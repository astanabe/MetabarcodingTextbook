\documentclass[titlepage,10pt,a4paper,english]{jsbook}

\setcounter{tocdepth}{4}
\setcounter{chapter}{-1}

\usepackage[round,colon,authoryear]{natbib}

\usepackage[dvipdfmx]{color,graphicx}

\usepackage[%
dvipdfm,%
pdfstartview={FitH -32768},%    描画領域の幅に合わせる
bookmarks=true,%                しおり付き
bookmarksnumbered=false,%        章や節の番号をふる
bookmarkstype=toc,%             目次情報のファイル.tocを参照
colorlinks=true,%              ハイパーリンクを色文字に
linkcolor=black,%       link の枠の色 black
citecolor=black,%       cite の枠の色 black
urlcolor=black,%        url の枠の色 black
pdftitle={Metabarcoding and DNA barcoding for Ecologists},%
pdfauthor={Akifumi S. Tanabe},
pdfkeywords={Metagenome, Environmental DNA, Next-Generation Sequencer, 454, IonTorrent, Illumina}%
]{hyperref}

\usepackage{pxfonts}

\bibliographystyle{econ}

\makeatletter
\def\maketitle{%
  \includegraphics[scale=1.2,keepaspectratio]{metabarcodingtextbook.en.eps}%
  \cleardoublepage
  \begin{titlepage}%
    \let\footnotesize\small
    \let\footnoterule\relax
    \let\footnote\thanks
    \null\vfil
    \vskip 60\p@
    \begin{center}%
      {\LARGE \@title \par}%
      \vskip 3em%
      {\large
        \lineskip .75em
        \begin{tabular}[t]{c}%
          \@author
        \end{tabular}\par}%
      \vskip 1.5em
      {\large \@date \par}%
    \end{center}%
    \par
    \@thanks\vfil\null
  \end{titlepage}%
  \setcounter{footnote}{0}%
  \global\let\thanks\relax
  \global\let\maketitle\relax
  \global\let\@thanks\@empty
  \global\let\@author\@empty
  \global\let\@date\@empty
  \global\let\@title\@empty
  \global\let\title\relax
  \global\let\author\relax
  \global\let\date\relax
  \global\let\and\relax
}
\makeatother

\title{Metabarcoding and DNA barcoding for Ecologists}
\author{Akifumi S. Tanabe}
\date{\today}

\renewcommand{\baselinestretch}{1.3}
\renewcommand{\prepartname}{Part }
\renewcommand{\postpartname}{}
\renewcommand{\prechaptername}{Chapter }
\renewcommand{\postchaptername}{}
\renewcommand{\presectionname}{}
\renewcommand{\postsectionname}{}
\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\listfigurename}{List of Figures}
\renewcommand{\listtablename}{List of Tables}
\renewcommand{\refname}{References}
\renewcommand{\bibname}{References}
\renewcommand{\indexname}{Index}
\renewcommand{\figurename}{Figure }
\renewcommand{\tablename}{Table }
\renewcommand{\appendixname}{Appendix }

\usepackage{float}
\usepackage{framed}
\definecolor{shadecolor}{gray}{0.9}
\newenvironment{content}{\begin{shaded}\vspace{-1em}\raggedright\ttfamily\footnotesize\setlength{\baselineskip}{1.4em}}{\end{shaded}\vspace{-1em}}
\newenvironment{pre}{\begin{leftbar}\raggedright\ttfamily\footnotesize\setlength{\baselineskip}{1.4em}}{\end{leftbar}\vspace{-1em}}
\newenvironment{cmd}{\begin{oframed}\raggedright\ttfamily\footnotesize\setlength{\baselineskip}{1.4em}}{\end{oframed}\vspace{-1em}}

\setlength{\textwidth}{\fullwidth}
\setlength{\evensidemargin}{\oddsidemargin}
\addtolength{\evensidemargin}{-2.5 true mm}
\addtolength{\oddsidemargin}{2.5 true mm}

\makeatletter
\renewcommand{\chapter}{%
  \if@openright\cleardoublepage\else\clearpage\fi
  \global\@topnum\z@
  \secdef\@chapter\@schapter}
\makeatother

\begin{document}
\thispagestyle{empty}
\maketitle
\cleardoublepage
\pagenumbering{roman}
\tableofcontents
\cleardoublepage
\setlength{\parindent}{0em}
\setlength{\parskip}{1em plus 0.2em}
\parindent=0em
\parskip=1em plus 0.2em
\pagenumbering{arabic}

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

I started the writing of this text to promote amplicon sequence assember ``Assams'' and utility programs package ``Claident'' for sequence clustering and identification.
In this text, I explain not only usage of these programs but also the methods and procedures of data collection and taxonomic identification by DNA barcoding.

Metabarcoding has been already widely used in bacterial reseaches, but it's utility is not limited to bacteria.
Metabarcoding is also applicable to soil fungi, fungi inhabiting the bodies of animals and plants, aquatic planktons, and environmental DNA emitted from macro-organisms.
I explain the methods of amplification of barcoding locus from environmental DNA or metagenomes, sequencing by high-throughput sequencers (a.k.a next-generation sequencers), taxonomic assignment of nucleotide sequences (a.k.a. DNA barcoding), and observation of presence/absence of operational taxonomic units.

This text is distributed under Creative-Commons Attribution-ShareAlike 2.1 Japan License.
You can copy, redistribute, display this text if you designate the authorship.
You can also modify this text and distribute the modified version if you designate the authorship and apply this license or compatible license to the modified version.
You can read the license at the following URL.\\
\href{http://creativecommons.org/licenses/by-sa/2.1/jp/}{http://creativecommons.org/licenses/by-sa/2.1/jp/}\\
You can also ask about this license to Creative-Commons, 171 Second Street, Suite 300, San Francisco, California 94105, USA.

I hope that this text helps you.
I am grateful to Dr. Hirokazu Toju (Graduate Shool of Human and Environmental Sciences, Kyoto University), Dr. Satoshi Nagai (National Research Institute of Fisheries Science, Japan Fisheries Research and Education Agency), and you.

\chapter*{Legends}
\addcontentsline{toc}{chapter}{Legends}

In this text, the input commands to terminals and display outputs are described as below.

\begin{cmd}
\# comments\\
{\textgreater} command argument1 {\textbackslash}\\
argument2 {\textbackslash}\\
argument3↓\\
output of command\\
{\textgreater} command argument1 argument2 argument3↓\\
output of command
\end{cmd}

In the above example, the same commands \texttt{command argument1 argument2 argument3} were executed twice.
The outputs \texttt{output of command} were displayed after execution.
The characters between \# and line feed were comments and needless to input.
\texttt{\textgreater} and space of line head indicate the prompt of terminal.
Do not type these characters.
↓ means the end of input commands and arguments and needless to input, but you need to type Enter key to input line feed.
I use line feed within commands or arguments for viewability.
Such line feed is led by \texttt{\textbackslash}.
Therefore, the line feeds led by \texttt{\textbackslash} do not mean the end of commands or arguments, or designation to input Enter key.
Involuntary line feeds may be generated by word wrap function depending on your read environment, but do not mean the end of commands or arguments, or designation to input Enter key.

The file content is shown as below in this text.

\begin{content}
| The content of first line\\
| The content of second line
\end{content}

\texttt{|} and space of line head indicate the line head in the file, do not exist in the file and needless to input these characters.
This code is written to help you to distinguish true line feeds and involuntary line feeds.

\chapter{Installing softwares and preparing analysis environment}

In this text, I assume Debian GNU/Linux jessie (hereafter Debian) or Ubuntu Linux 14.04 LTS (hereafter Ubuntu) as operating system.
If you use Windows PC, please install Debian or Ubuntu.
Cygwin or Windows Subsystem for Linux provided for Windows 10 build 14393 or later can be used for the following analysis, but the programs run much more slowly.
You can use CD, DVD or USB memory to boot installer of Linux.
If your PC has only one storage device, you need to reduce Windows partition by using partition resizer software such as EaseUS Partition Master or using a partition resize function contained in the installer.
You can also use newly added internal storage devices or external storage devices connected by USB.
There are several variations of Ubuntu, and I recommend Xubuntu rather than normal Ubuntu.

Debian and Ubuntu can be installed to Mac.
If there is no enough space, you need to resize OSX partition with the aid of Disk Utility or add storage device.
The rEFIt or rEFInd boot selecter may be required to boot Debian, Ubuntu or the installer of them on Mac.
If you install rEFIt or rEFInd to your Mac, you can boot the installer of Debian or Ubuntu from CD, DVD or USB memory.
Do not delete existing partition of OSX.
If you have enough free space, you don't need to use Disk Utility to resize existing partition.
You can install Debian or Ubuntu to external storage devices on Mac.

I assume Intel64/AMD64 (x86{\textunderscore}64) CPU machine as analysis environment.
The other CPU machine can be used for analysis, but you need to solve problems by yourself.
The 64 bits version of Debian or Ubuntu is also required because 32 bits version cannot use large memory.

\section{Installation of Claident, Assams, databases, and the other required programs}

Run the following commands in terminal or console as the user that can use \texttt{sudo}.
Then, all of the required softwares will be installed.
The installer will ask password to you when \texttt{sudo} is used.

\begin{cmd}
{\textgreater} mkdir -p {\textasciitilde}/workingdirectory↓\\
{\textgreater} cd {\textasciitilde}/workingdirectory↓\\
{\textgreater} wget https://www.claident.org/installClaident{\textunderscore}Debian.sh↓\\
{\textgreater} sh installClaident{\textunderscore}Debian.sh↓\\
{\textgreater} wget https://www.claident.org/installOptions{\textunderscore}Debian.sh↓\\
{\textgreater} sh installOptions{\textunderscore}Debian.sh↓\\
{\textgreater} wget https://www.claident.org/installDB{\textunderscore}Debian.sh↓\\
{\textgreater} sh installDB{\textunderscore}Debian.sh↓\\
{\textgreater} wget https://www.claident.org/installUCHIMEDB{\textunderscore}Debian.sh↓\\
{\textgreater} sh installUCHIMEDB{\textunderscore}Debian.sh↓\\
{\textgreater} cd ..↓\\
{\textgreater} rm -r workingdirectory↓
\end{cmd}

By default, the softwares will be installed to \texttt{/usr/local}.
In the installation, you will see \texttt{Permission denied} error and the installer ask password to you.
If the installer continue after password input, you don't need to care about the error.
The installer try to install without \texttt{sudo} at first and the installation output the above error.
Then, the installer try to install using \texttt{sudo}.

If you need proxy to connect the internet, execute the following commands to set environment variables before execution of the installer.

\begin{cmd}
{\textgreater} export http{\textunderscore}proxy=http://server.address:portnumber↓\\
{\textgreater} export https{\textunderscore}proxy=\$http{\textunderscore}proxy↓\\
{\textgreater} export ftp{\textunderscore}proxy=\$http{\textunderscore}proxy↓\\
{\textgreater} export all{\textunderscore}proxy=\$http{\textunderscore}proxy↓
\end{cmd}

If the proxy requires username and password, execute the following commands instead of the above commands.

\begin{cmd}
{\textgreater} export http{\textunderscore}proxy=http://username:password@server.address:portnumber↓\\
{\textgreater} export https{\textunderscore}proxy=\$http{\textunderscore}proxy↓\\
{\textgreater} export ftp{\textunderscore}proxy=\$http{\textunderscore}proxy↓\\
{\textgreater} export all{\textunderscore}proxy=\$http{\textunderscore}proxy↓
\end{cmd}

\subsection{Upgrading to new version}

If you want to upgrade all of the softwares and the databases, run the same commands as initial installation.
By this procedure, Assams, Claident, PEAR, VSEARCH, Metaxa and ITSx will be installed to \texttt{/usr/local}, and NCBI BLAST+, BLAST databases for molecular identification, taxonomy databases and the other required programs will be installed to \texttt{/usr/local/share/claident}.
NCBI BLAST+ and BLAST databases used by Claident can co-exist system wide installation of NCBI BLAST+ and BLAST databases.

You can disable a part of upgrade like below.

\begin{cmd}
{\textgreater} mkdir -p {\textasciitilde}/workingdirectory↓\\
{\textgreater} cd {\textasciitilde}/workingdirectory↓\\
\# disable upgrade of Assams\\
{\textgreater} touch .assams↓\\
\# disable upgrade of Claident\\
{\textgreater} touch .claident↓\\
\# disable upgrade of PEAR\\
{\textgreater} touch .pear↓\\
\# disable upgrade of VSEARCH\\
{\textgreater} touch .vsearch↓\\
\# disable upgrade of NCBI BLAST+\\
{\textgreater} touch .blast↓\\
\# execute upgrade\\
{\textgreater} wget https://www.claident.org/installClaident{\textunderscore}Debian.sh↓\\
{\textgreater} sh installClaident{\textunderscore}Debian.sh↓\\
\# disable upgrade of sff{\textunderscore}extract\\
{\textgreater} touch .sffextract↓\\
\# disable upgrade of HMMer\\
{\textgreater} touch .hmmer↓\\
\# disable upgrade of MAFFT\\
{\textgreater} touch .mafft↓\\
\# disable upgrade of Metaxa\\
{\textgreater} touch .metaxa↓\\
\# disable upgrade of ITSx\\
{\textgreater} touch .itsx↓\\
\# execute upgrade\\
{\textgreater} wget https://www.claident.org/installOptions{\textunderscore}Debian.sh↓\\
{\textgreater} sh installOptions{\textunderscore}Debian.sh↓\\
\# disable upgrade of ``overall'' BLAST and taxonomy databases\\
{\textgreater} touch .overall↓\\
\# execute upgrade\\
{\textgreater} wget https://www.claident.org/installDB{\textunderscore}Debian.sh↓\\
{\textgreater} sh installDB{\textunderscore}Debian.sh↓\\
\# disable upgrade of ``Claident Databases for UCHIME''\\
{\textgreater} touch .cdu↓\\
\# disable upgrade of ``rdp'' reference database for chimera detection\\
{\textgreater} touch .rdp↓\\
\# disable upgrade of ``silva'' reference databases for chimera detection\\
{\textgreater} touch .silva↓\\
\# disable upgrade of ``unite'' reference databases for chimera detection\\
{\textgreater} touch .unite↓\\
\# execute upgrade\\
{\textgreater} wget https://www.claident.org/installUCHIMEDB{\textunderscore}Debian.sh↓\\
{\textgreater} sh installUCHIMEDB{\textunderscore}Debian.sh↓\\
{\textgreater} cd ..↓\\
{\textgreater} rm -r workingdirectory↓
\end{cmd}

\subsection{Installing to non-default path}

If you install the softwares based on the above procedure, the softwares will be installed to \texttt{/usr/local}.
The executable commands will be installed to \texttt{/usr/local/bin}.
You can change these install path for coexistence with the other programs such as older versions like below.

\begin{cmd}
{\textgreater} mkdir -p {\textasciitilde}/workingdirectory↓\\
{\textgreater} cd {\textasciitilde}/workingdirectory↓\\
{\textgreater} export PREFIX=install{\textunderscore}path↓\\
{\textgreater} wget https://www.claident.org/installClaident{\textunderscore}Debian.sh↓\\
{\textgreater} sh installClaident{\textunderscore}Debian.sh↓\\
{\textgreater} wget https://www.claident.org/installOptions{\textunderscore}Debian.sh↓\\
{\textgreater} sh installOptions{\textunderscore}Debian.sh↓\\
{\textgreater} wget https://www.claident.org/installDB{\textunderscore}Debian.sh↓\\
{\textgreater} sh installDB{\textunderscore}Debian.sh↓\\
{\textgreater} wget https://www.claident.org/installUCHIMEDB{\textunderscore}Debian.sh↓\\
{\textgreater} sh installUCHIMEDB{\textunderscore}Debian.sh↓\\
{\textgreater} cd ..↓\\
{\textgreater} rm -r workingdirectory↓
\end{cmd}

In this case, the following commands need to be executed before analysis.

\begin{cmd}
{\textgreater} export PATH=install{\textunderscore}path/bin:\$PATH↓
\end{cmd}

You can omit above command if the above command is added to \texttt{{\textasciitilde}/.bash{\textunderscore}profile} or \texttt{{\textasciitilde}/.bashrc}.

\subsection{How to install multiple versions in a computer}

If you install Claident and the other softwares to default install path of a computer to which Claident was already installed, all softwares will be overwritten.
As noted above, multiple versions of Claident can coexist if you install Claident to non-default path.
Note that a configuration file \texttt{.claident} placed at a home directory of login user (\texttt{/home/username}) or \texttt{/etc/claident} cannot coexist at the same path.
You need to replace this file before changing the version of Claident.
The configuration file at the home directory of login user will be used preferentially.
To use multiple version, I recommend to make user account for each version and to install Claident to the home directory of each user.
Then, the version of Claident can be switched by switching login user.

\chapter{Sequencing of multiple samples by next-generation sequencers}

In this chaper, I explain brief overview of tagged multiplex sequencing method by Roche GS series sequencers, Ion PGM and Illumina MiSeq.
These sequencers can read over-400bp contiguously and are suitable for metabarcoding and DNA barcoding.
Note that MiSeq requires concatenation of paired-end reads.
Therefore, PCR amplicons should be 500bp or shorter (400bp is recommended) in order to concatenate paired-end reads.
Forward and reverse reads can be analyzed separetely, but I cannot recommend such analysis because reverse reads are usually low quality.

The next-generation sequencers output extremely large amount of nucleotide sequences in single run.
Running costs of single run is much higher than Sanger method-based sequencers.
To use such sequencers efficiently, multiplex sequencing method was developed.
Multiplex identifier tag sequences are added to target sequences to identify the sample of origin, and the multiple tagged samples are mixed and sequenced in single run in this method.
This method can extremely reduce per-sample sequencing costs.
Multiplex identifier tag is also called as ``barcode''.
However, nucleotide sequence for DNA barcoding is called as ``barcode sequence''.
This is very confusing and ``multiplex identifier tag'' is too long.
Thus, I call multiplex identifier tag sequence as just ``tag'' in this text.
Please notice that tag is often called as ``index''.

In the following analysis, chimera sequences constructed in PCR and erroneous sequences potentially causes misinterpretation of analysis results.
If multiple PCR replicates are prepared, tagged and sequenced separately, shared sequences among all replicates can be considered as nonchimeric and less erroneous.
This is because there are huge number of sequence combinations and joint points but no error sequence pattern is only one for one true sequence and nonchimeric and no error sequences likely to be observed at all replicates.
Program cannot remove chimeras and errors enough but we can expect that the combination of PCR replicates and program improves removal efficiency of chimeras and errors.
After removal of chimeras and errors, the number of sequences of PCR replicates can be summed up and used in subsequent analysis.

\section{PCR using tag- and adapter-jointed-primers}

In order to add tag to amplicon, PCR using tag-jointed primer is the easiest way.
This method requires a set of tag-jointed primers.
In addition, library preparation kits for next-generation sequencers usually presume that the adapter sequences specified by manufacturers are added to the both end of target sequences.
Thus, the following tag- and adapter-jointed primer is used for PCR.

\begin{pre}
5' ― [adapter] ― [tag] ― [specific primer] ― 3'
\end{pre}

If this kind of primers are used for the both forward and reverse primers, the following amplicon sequences will be constructed.

\begin{pre}
5' ― [adapter-F] ― [tag-F] ― [specific primer-F] ― [target sequence] ― [specific primer-R (reverse complement)] ― [tag-R (reverse complement)] ― [adapter-R (reverse complement)] ― 3'
\end{pre}

In the case of single-end read, tag-F leads specific primer-F and target sequence in the sequence data.

The supplement of \citet{Hamady2008} may be useful for picking tag sequences.
In the case of single-end sequencing, 3'-side tag is not required, and tagless primer can be used for PCR.
In the case of paired-end sequencing, single index (tag) can be applied, but dual index (tag) is recommended for detecting unlikely tag combinations which means that forward and reverse sequences are mispaired.

Using above primer sets for PCR, primers anneal to templates in ``Y''-formation, and the amplicon sequences which have tags and adapters for both ends will be constructed.
Then, the amplicon solutions are mixed in the same concentration and sequenced based on manufacturer's protocol.
Spectrophotometer (including Nanodrop) is inappropreate for the measurement of the concentration of solution because measurement of dsDNA using spectrophotometer is likely to be affected by the other contaminants.
I recommend Qubit (ThermoFisher) for measurement of dsDNA concentration.
Quantitative PCR-based method can also be recommended but it's expensive and more time-consuming.

Primer annealing position sequence can also be used for recognizing the sample of origin.
Therefore, the sequences of multiple loci, for example plant \textit{rbcL} and \textit{matK}, from same sample set tagged by same tag set can be multiplexed and sequenced.
Of course, the sequences of multiple loci can also be recognized by themselves.
Smaller number of cycles and longer extension time were recommended for PCR.
Because the required amount of DNA for sequence sample preparation is not so high, the larger number of cycles of PCR amplification is not needed.
The larger number of cycles and shorter extension time generates more incompletely extended amplicon sequences and the incompletely extended amplicon sequences are re-extend using different template sequences in next cycle.
Such sequences are called as ``chimeric DNA''.
Chimeric DNAs causes a discovery of non-existent novel species or a overestimation of species diversity.
To reduce chimeric DNA construction, using high-fidelity DNA polymerase such as Phusion (Finnzymes) or KOD (TOYOBO) is effective.
\citet{Stevens2013} reported that slowing cooling-down from denaturation temperature to annealing temperature reduced chimeric DNA construction.
If your thermal cycler can change cooling speed, slowing cooling-down from denaturation temperature to annealing temperature can be recommended.
Chimeric DNA sequences can also be eliminated by computer programs after sequencing.
Because chimera removal by programs is incomplete and the nonchimeric sequences shrink, we cannot do better than reduce chimeric DNA construction.

In the case of hardly amplifiable templates, using Ampdirect Plus (Shimadzu) for PCR buffer or crushing by homogenizer or beads before DNA extraction is recommended.
Deep freezing before crushing can also be recommended.
Removal of polyphenols or polysaccharides might be required if your sample contain those chemicals.
If PCR amplification using tag- and adapter-jointed-primers fail, try two-step PCR that consist of primary PCR (20--30 cycles) using primers without tags and adapters, purification of amplicons by ExoSAP-IT, and secondary PCR (5--10cycles) using amplicons of primary PCR as templates and tag- and adapter-jointed-primers.

\subsection{Decreasing costs by interim adapters}\label{subsection:interimadapter}

Tag- and adaper-jointed-primers are very long and expensive.
In addition, we need to buy tag- and adaper-jointed-primers for each locus.
To reduce cost of tag- and adaper-jointed-primers, interim adapter-jointed primers and two-step PCR is useful.
The following primer set is used in primary PCR.

\begin{pre}
5' ― [interim adapter] ― [specific primer] ― 3'
\end{pre}

This PCR product have interim adapter sequences at the both ends.
This PCR product is used as template in secondary PCR after purification.
The following primer set is used in secondary PCR.

\begin{pre}
5' ― [adapter specified by manufacturer] ― [tag] ― [interim adapter] ― 3'
\end{pre}

This two-step PCR enables us to reuse secondary PCR primers.
However, this two-step PCR may increase PCR errors and PCR amplification biases, and decrease target sequence lengths.
Note that final PCR product is constructed as the following style.

\begin{pre}
5' ― [adapter-F specified by manufacturer] ― [tag-F] ― [interim adapter-F] ― [specific primer-F] ― [target sequence] ― [specific primer-R (reverse complement)] ― [interim adapter-R (reverse complement)] ― [tag-R (reverse complement)] ― [adapter-R (reverse complement) specified by manufacturer] ― 3'
\end{pre}

Illumina's multiplex sequencing method \citep{Illumina2013} using Nextera XT Index Kit is same as the above method.
In the dual-index paired-end sequencing based on this method, the first read start from behind of interim adapter-F (i.e. head of specific primer-F) to target sequence.
The second read start from behind of interim adapter-R and contains tag-R (index1) sequence.
The third read start from behind of adapter-F and contains tag-F (index2) sequence.
The last read start from ahead of interim adapter-R (i.e. tail of specific primer-R) to target sequence.
The first, second, third and last reads are saved as \texttt{*{\textunderscore}R1{\textunderscore}*.fastq.gz}, \texttt{*{\textunderscore}R2{\textunderscore}*.fastq.gz}, \texttt{*{\textunderscore}R3{\textunderscore}*.fastq.gz} and \texttt{*{\textunderscore}R4{\textunderscore}*.fastq.gz}, respectively.
The first, second and third reads are same strand, but last read is reverse strand.
Because the sequencing primers for the first and the last reads are targeting interim adapter-F and interim adapter-R, respectively, the first and the last reads contains the sequences of specific primer-F and specific primer-R, respectively.
Thus, the target sequences contained in the first and the last reads are shrinked.
If the length of the target sequence is 500 bp or longer, there might be no overlap and paired-end reads cannot be concatenate.
If specific primer-F and specific primer-R are used as sequencing primers for the first and the last reads, you can exclude the sequences of specific primer-F and specific primer-R from the first and the last reads.
However, the following quality improvement method by insertion of N cannot be applied in such case.

\subsection{Quality improvement by insertion of N}

On the Illumina platform, luminescence of syntheses of DNA on a flowcell is detected by optical sensor.
PCR amplicons of metagenomes are single locus and much more homogeneous than genome shotgun or RNA-seq library sequences.
In such case, neighboring sequences on a flowcell is difficult to distinguish one from the other.
In addition, if the nucleotide of the most sequences (especially first 12 nucleotides) are the same and nonluminescence, the Illumina platform sequencer will determined as failure and crash.
To avoid this problem, insertion of \texttt{NNNNNN} between specific primer and interim adapter is effective.
\texttt{NNNNNN} of the head of sequences enables sequencers to distinguish neighboring sequences and prevent black out, and the sequencing quality therefore will be improved \citep{Nelson2014}.
The varied length of \texttt{NNNNNN} causes artificial frameshift and also effective \citep{Fadrosh2014}.
PhiX control can be reduced by using the above methods, and the application sequences will increase.

\chapter{Preprocessing of nucleotide sequence data}

Roche GS series sequencers and Ion PGM output raw sequencing data as \texttt{*.sff}.
Illumina platform sequencers output \texttt{*.fastq} files.
In this chapter, the procedures of demultiplexing, quality-trimming and quality-filtering.
The \texttt{clsplitseq} command of Claident is recommended for demultiplexing because the programs provided by manufacturer ignores the quality of tag positions.
The following commands should be executed in the terminal or console.
Fundamental knowledge of terminal operations is required.
If you are unfamiliar with terminal operations, you need to become understandable about the contents of appendix \ref{chapter:terminalcommands}.

\section{Importing sequence data deposited to SRA/DRA/ERA or demultiplexed FASTQ}

Claident assumes \texttt{SequenceID{\textunderscore}{\textunderscore}RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID} for definition lines of sequences, and \texttt{RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID} for file names (without extension).
Therefore, the sequence data deposited to SRA/DRA/ERA or demultiplexed FASTQ cannot be used as is.
The \texttt{climportfastq} of Claident can convert such data.
If your data is paired-end, you need to concatenate and filter the sequences before conversion (see section \ref{subsection:concatenatingpairedend}).
The following plain text file is required for conversion.

\begin{content}
| SequenceFileName1 RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID\\
| SequenceFileName2 RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID\\
| SequenceFileName3 RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID
\end{content}

Dummy RunID and PrimerID is acceptable.
PrimerID need to be the same among the sample used the same primer set.
TagID need to be different among the different sample files.
TagID can be the same as the sequence file name.

After the above file was prepared, execute \texttt{climportfastq} like the following and the above file should be given as an input file.

\begin{cmd}
{\textgreater} climportfastq {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

Then, you can find converted files in the output folder.
If your sequence data is single-end, quality filtering explained in section \ref{subsection:qualityfilteringfor454} is recommended.

\section{For Roche GS series sequencers and Ion PGM}

\subsection{Converting SFF to FASTQ}

First of all, conversion of raw SFF format file to FASTQ file is needed like the following.

\begin{cmd}
{\textgreater} sff{\textunderscore}extract -c inputfile(SFF)↓
\end{cmd}

\texttt{-c} argument enables trimming of \texttt{TCAG} at the head of sequences.
If you add \texttt{TCAG} to the head of tag sequences, do not use this argument.
Assuming your SFF file name is \texttt{HOGEHOGE.sff}, \texttt{HOGEHOGE.fastq} will be saved as FASTQ file.
\texttt{HOGEHOGE.xml} will also be generated, but this is not required.
The output sequences have tag sequences at the beginning, followed by primer-F and target sequences, and primer-R (reverse complement) at the end.
Note that all sequences are not completely read from the beginning to the end, the incomplete sequences are included.
The \texttt{sff{\textunderscore}extract} command is used in this book, but any other programs which can clip \texttt{TCAG} at the beginning can be used.
If the SFF to FASTQ converter program cannot clip \texttt{TCAG} at the beginning, adding \texttt{TCAG} to the beginning of tag sequences to give to \texttt{clsplitseq} also works well, but the quality values will be strictly checked.

\subsection{Demultiplexing of sequences}

The FASTQ file that contain the sequences from multiple samples need to be demultiplexed based on tag sequences and primer sequences before the subsequent analysis.
To do this process, a FASTA file which contain tag sequences and another FASTA file which contain primer-F sequences are required.

\begin{content}
| {\textgreater}TagID\\
| [tag sequence]\\
| {\textgreater}examplesample1\\
| ACGTACGT
\end{content}

\begin{content}
| {\textgreater}PrimerID\\
| [primer sequence]\\
| {\textgreater}exampleprimer1\\
| ACGTACGTACGTACGTACGT
\end{content}

Degenerate codes of nucleotides are not allowed for tag sequences, but those are allowed for primer sequences.
Both of tag and primer FASTQ files can contain multiple sequences.
If you use interim adapter explained in section \ref{subsection:interimadapter}, primer sequences should be written like the following.

\begin{content}
| {\textgreater}PrimerID\\
| [interim adapter][primer sequence]\\
| {\textgreater}exampleprimer1\\
| TGATACTCGATACGTACGTACGTACGTACGTACGT
\end{content}

Thus, the sequences between tag and target sequences should be written in primer FASTA file.

All the above files are prepared, the following command can demultiplex nucleotide sequences to each sample FASTQ file.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
{-}{-}minqualtag=27 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

RunID must differ among different sequencing runs.
RunID is given by sequencer in many cases, you can use such sequencer generated RunID.
RunID is usually contained in sequence file name or sequence name in sequence file, but the naming rules are different among sequencing platforms.
Therefore, \texttt{clsplitseq} requires RunID given by user.
\texttt{{-}{-}minqualtag} is an argument that specifies minimum quality threshold of tag position sequences.
If 1 or more lower quality nucleotide than this threshold value is contained by a sequence, such sequence will be omitted from output sequences.
27 for minimum quality threshold is proposed by \citet{Kunin2010} for 3'-tail trimming of the sequences of Roche GS series sequencers.
The different value might be more suitable for the other sequencers.
In many cases, 30 is used for minimum quality threshold and can be recommended.

If multiplex sequencing technique is not used, \texttt{{-}{-}tagfile} argument can be omitted.
However, just omit of \texttt{{-}{-}tagfile} generates incompatible FASTQ files for Claident.
In such case, you should add identifier (dummy is acceptable) of tag sequences using \texttt{{-}{-}indexname=TagID} argument.

The tag and primer position sequences are trimmed from the output sequences.
Tag position sequence match is evaluated exactly and strictly.
There are no arguments to tolerate a mismatch.
Primer position sequence is aligned based on Needleman-Wunsch algorithm and evaluated allowing 14\% of mismatches (the threshold can be changed).
The output files are named as \texttt{RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.fastq.gz} and saved in the output folder.
\texttt{clsplitseq} can use multiple CPUs for faster processing.
If your computer have 4 CPU cores, 4 should be speficied for \texttt{{-}{-}numthreads} argument.
Note that operating system and/or writing speed of storage devices might limit processing speed.
By default, the output files are compressed by GZIP.
Therefore, decompression is required to read/write by incompatible programs with gzipped FASTQ files.
The commands of Claident used below can treat gzipped FASTQ files.

Before submission of manuscripts, sequence data need to be deposited to public database such as DDBJ Sequence Read Archive (DRA).
Gzipped FASTQ files in this step can be used for the data deposition.

\subsubsection{If you sequenced a number of samples by multiple sequencing runs}
Multiple demultiplexing by \texttt{clsplitseq} are required.
However, \texttt{clsplitseq} cannot write already existing folder by default.
The secondary run of \texttt{clsplitseq} requires \texttt{{-}{-}append} argument like below.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
{-}{-}minqualtag=27 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
outputfolder↓\\
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
{-}{-}minqualtag=27 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
{-}{-}append {\textbackslash}\\
inputfile2 {\textbackslash}\\
outputfolder↓
\end{cmd}

\subsubsection{If your tag sequence lengths are unequal}
\texttt{clsplitseq} assumes that all tag sequence lengths are equal for faster processing.
The unequal length tags must be splitted to multiple tag sequence files and multiple demultiplexing runs of \texttt{clcplitseq} are required as the following.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile1 {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
{-}{-}minqualtag=27 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓\\
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile2 {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
{-}{-}minqualtag=27 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
{-}{-}append {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

\subsubsection{Recognition and elimination of reverse primer positions}
In the above procedure, reverse primer position and subsequent sequences are not eliminated.
Reverse primer position and subsequent sequences are artificial and should be eliminated if possible.
To do so, reverse primer sequence file like the following is required.

\begin{content}
| {\textgreater}PrimerID\\
| [primer sequence]\\
| {\textgreater}exampleprimer1\\
| TCAGTCAGTCAGTCAGTCAG
\end{content}

Multiple reverse primers can written in this file.
Note that the N-th reverse primer sequence is assumed to associate with the N-th forward primer sequence.
Therefore, the different number of primer sequences between forward and reverse primer sequence files causes an error.
If there are the samples whose forward or reverse primer sequence is same but the other primer sequence is different, both combinations of forward and reverse primer sequences need to be written as different primers in the files.

After the preparation of the above file, perform \texttt{clsplitseq} as the following.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile {\textbackslash}\\
{-}{-}primerfile=ForwardPrimerSequenceFile {\textbackslash}\\
{-}{-}reverseprimerfile=ReversePrimerSequenceFile {\textbackslash}\\
{-}{-}reversecomplement {\textbackslash}\\
{-}{-}minqualtag=27 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

In this processing, reverse-complement sequence of reverse primer is searched based on Needleman-Wunsch algorithm allowing 15\% (this value can be changed) of mismatches and reverse primer position and subsequent sequence is eliminated in addition to the above process.
If reverse-complement sequence of reverse primer is not found and the other requirement is fullfilled, the sequence will be saved to output file by default.
The \texttt{{-}{-}needreverseprimer} argument is required to filter out the sequence which does not contain reverse-complement sequence of reverse primer.

\subsection{Trimming low quality tail and filtering low quality sequences}\label{subsection:qualityfilteringfor454}

FASTQ sequences have read quality information.
The low quality 3'-tail can be trimmed and the low quality sequences can be filtered out based on the quality values.
The \texttt{clfilterseq} command can perform such processing as the following.

\begin{cmd}
{\textgreater} clfilterseq {\textbackslash}\\
{-}{-}minqual=27 {\textbackslash}\\
{-}{-}minquallen=3 {\textbackslash}\\
{-}{-}minlen=350 {\textbackslash}\\
{-}{-}maxlen=400 {\textbackslash}\\
{-}{-}maxplowqual=0.1 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

The values of \texttt{{-}{-}minqual} and \texttt{{-}{-}minquallen} indicate the minimum threshold of read quality value and size of sliding window, respectively.
The above command trims 3'-tail positions until 3 bp long sequence whose read quality is 27 or higher in all 3 positions are observed.
In addition, trimmed sequences shorter than \texttt{{-}{-}minlen} will be filtered out and trimmed sequences longer than \texttt{{-}{-}maxlen} will be trimmed to \texttt{{-}{-}maxlen}.
The remaining sequences containing \texttt{{-}{-}maxplowqual} or more rate of lower quality positions than \texttt{{-}{-}minqual} will also be filtered out.
The output is a file by default, but can be saved to the file in the new folder using \texttt{{-}{-}output=folder} argument.
The output file name is same as the input file name in this case.
If you want to save the output files to the existing folder, add \texttt{{-}{-}append} argument.

If you want to apply \texttt{clfilterseq} to the all files in the output folder of \texttt{clsplitseq}, run the following command.

\begin{cmd}
{\textgreater} for f in OutputFolderOfclsplitseq/*.fastq.gz↓\\
do clfilterseq {\textbackslash}\\
{-}{-}output=folder {\textbackslash}\\
{-}{-}append {\textbackslash}\\
{-}{-}minqual=27 {\textbackslash}\\
{-}{-}minquallen=3 {\textbackslash}\\
{-}{-}minlen=350 {\textbackslash}\\
{-}{-}maxlen=400 {\textbackslash}\\
{-}{-}maxplowqual=0.1 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
\$f {\textbackslash}\\
outputfolder↓\\
done↓
\end{cmd}

\section{For Illumina platform sequences}

\subsection{Converting from BCL to FASTQ}

The analysis software of Illumina platform sequences can demultiplex sequencing reads, but ignores read quality of tag positions.
Therefore, the sequences have low quality tag positions possibly saved to demultiplexed FASTQ.
To filering out such sequences, pre-demultiplexed FASTQ files are required and can be converted from BCL files with the aid of bcl2fastq.
There are 1.x and 2.x series of bcl2fastq and both series can be used for Claident.
However, the sequencers may be compatible to either 1.x or 2.x, you need to select proper version.
Pre-demultiplexed FASTQ can be demultiplexed by \texttt{clsplitseq} in Claident.
See appendix to install bcl2fastq.

To convert BCL to FASTQ, run data folder (superjacent folder of BaseCalls folder) need to be copied to the PC installed bcl2fastq.
If there is \texttt{SampleSheet.csv} in run data folder, this file must be renamed or deleted.

In the case of bcl2fastq 1.x, the following commands make FASTQ files from BCL files of 8 bp dual indexed 300PE sequencing data.

\begin{cmd}
{\textgreater} cd RunDataFolder↓\\
{\textgreater} configureBclToFastq.pl {\textbackslash}\\
{-}{-}fastq-cluster-count 0 {\textbackslash}\\
{-}{-}use-bases-mask Y300n,Y8,Y8,Y300n {\textbackslash}\\
{-}{-}input-dir BaseCalls {\textbackslash}\\
{-}{-}output-dir outputfolder↓\\
{\textgreater} cd outputfolder↓\\
{\textgreater} make -j4↓
\end{cmd}

The \texttt{{-}{-}fastq-cluster-count 0} argument disable large output file splitting.
The \texttt{{-}{-}use-bases-mask Y300n,Y8,Y8,Y300n} is an argument to save forward 300 bp read (last base is trimmed), 8 bp index 1 (reverse-complement of tag-R), 8 bp index 2 (tag-F) and reverse 300 bp read (last base is trimmed) to \texttt{*{\textunderscore}R1{\textunderscore}001.fastq.gz}, \texttt{*{\textunderscore}R2{\textunderscore}001.fastq.gz}, \texttt{*{\textunderscore}R3{\textunderscore}001.fastq.gz} and \texttt{*{\textunderscore}R4{\textunderscore}001.fastq.gz}, respectively.
The value of \texttt{{-}{-}use-bases-mask} argument need to be changed for the other sequencing settings.
For 6 bp single indexed 250SE and 8 bp dual indexed 300SE sequencing data, \texttt{{-}{-}use-bases-mask Y250n,Y6} and \texttt{{-}{-}use-bases-mask Y300n,Y8,Y8} should be suitable, respectively.
\texttt{make -j4} executes the conversion using 4 CPUs.
The output files will be compressed by GZIP.
The extension \texttt{.gz} of output files indicates that the file is compressed by GZIP.
Claident is compliant with gzipped FASTQ files and decompression is not required.

In the case of bcl2fastq 2.x, perform the following command.

\begin{cmd}
{\textgreater} bcl2fastq {\textbackslash}\\
{-}{-}processing-threads NumberOfCPUs {\textbackslash}\\
{-}{-}create-fastq-for-index-reads {\textbackslash}\\
{-}{-}use-bases-mask Y300n,I8,I8,Y300n {\textbackslash}\\
{-}{-}runfolder-dir RunDataFolder {\textbackslash}\\
{-}{-}output-dir outputfolder↓
\end{cmd}

The \texttt{{-}{-}processing-threads}, \texttt{{-}{-}use-bases-mask} and \texttt{{-}{-}runfolder-dir} indicate the number of processor used in conversion, masking option (almost same as 1.x but index length must be given as \texttt{I[number]} instead of \texttt{Y[number]}) and run data folder, respectively.

\subsection{Demultiplexing of sequences}

FASTA files containing tag (index) sequences and primer sequences like the following are needed for demultiplexing.
FASTA files containing secondary tag (index) sequences and reverse primer sequences are also required for paired-end sequencing data.

\begin{content}
| {\textgreater}TagID\\
| [tag sequence]\\
| {\textgreater}examplesample1\\
| ACGTACGT
\end{content}

\begin{content}
| {\textgreater}PrimerID\\
| [primer sequence]\\
| {\textgreater}exampleprimer1\\
| ACGTACGTACGTACGTACGT
\end{content}

Degenerate code is not allowed for tag sequences, but can be used in primer sequences.
Multiple tags and primers can be written in the files, but the N-th reverse tag/primer sequence is assumed to associate with the N-th forward tag/primer sequence.
Therefore, the different number of tag/primer sequences between forward and reverse tag/primer sequence files causes an error.
If there are the samples whose forward or reverse tag/primer sequence is same but the other tag/primer sequence is different, both combinations of forward and reverse tag/primer sequences need to be written as different tags/primers in the files.
If you added \texttt{N} in front of primer, \texttt{N} need to be added in primer sequence.
If your \texttt{N} length is unequal, only the longest \texttt{N} should be written in the file.

All the required files prepared, the following command demultiplex sequences to each sample file.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
{-}{-}runname=RunID {\textbackslash}\\
{-}{-}index1file=Index1Sequence(tag-Rrevcomp)File {\textbackslash}\\
{-}{-}index2file=Index2Sequence(tag-F)File {\textbackslash}\\
{-}{-}primerfile=ForwardPrimerSequenceFile {\textbackslash}\\
{-}{-}reverseprimerfile=ReversePrimerSequenceFile {\textbackslash}\\
{-}{-}minqualtag=30 {\textbackslash}\\
{-}{-}numthreads=NumberofCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
inputfile2 {\textbackslash}\\
inputfile3 {\textbackslash}\\
inputfile4 {\textbackslash}\\
outputfolder↓
\end{cmd}

The input files should be specified in the order of \texttt{*{\textunderscore}R1{\textunderscore}001.fastq.gz}, \texttt{*{\textunderscore}R2{\textunderscore}001.fastq.gz}, \texttt{*{\textunderscore}R3{\textunderscore}001.fastq.gz} and \texttt{*{\textunderscore}R4{\textunderscore}001.fastq.gz}.
The \texttt{{-}{-}index1file} and \texttt{{-}{-}index2file} arguments requires the FASTA sequence files of index 1 (reverse-complement of tag-R) and index 2 (tag-F), respectively.
By default, the acceptable mismatches are 14\% and 15\% for forward and reverse primers, respectively.
If you added \texttt{N} in front of primer, the \texttt{{-}{-}truncateN=enable} argument need to be given.
This argument enables exclusion of \texttt{N} of primer and matched positions of sequences in calculation of the rate of mismatches.
Therefore, only the longest \texttt{N} is required to find \texttt{N}-added primer even if the length of \texttt{N} is unequal.
After the processing, the number of sequences in demultiplexed files should be compared with those in demultiplexed files generated by Illumina softwares.
Correctly demultiplexed files should contain fewer sequences than demultiplexed files generated by Illumina softwares.
If you used specific primers for sequencing primers, forward and reverse sequences do not contain specific primer positions.
In such cases, \texttt{{-}{-}primerfile} and \texttt{{-}{-}reverseprimerfile} arguments are not required, but \texttt{{-}{-}primername=PrimerID} argument need to be given for converting sequence names as compliant with Claident.
Dummy PrimerID is acceptable but no PrimerID is not.

If you do not perform multiplex sequencing using tag/index, \texttt{{-}{-}index1file} and \texttt{{-}{-}index2file} arguments are unneeded, but \texttt{{-}{-}indexname=TagID} argument must be given for converting sequence names as compliant with Claident.
Dummy TagID is acceptable but no TagID is not.

After demultiplexing, \texttt{RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.forward.fastq.gz} and \texttt{RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.reverse.fastq.gz} will be generated.
These gzipped FASTQ files can be used for data deposition to sequence read archive sites such as DDBJ Sequence Read Archive (DRA).
In deposition process to DRA, it is required that the sequence lengths are equal or not.
Because primer position sequences that can be unequal lengths even if only one primer set was used are eliminated from demultiplexed sequence files, do not specify that the sequence lengths are equal.

\subsection{Concatenating forward and reverse sequences}\label{subsection:concatenatingpairedend}

\subsubsection{In the case of overlapped paired-end}

The \texttt{clconcatpair} command in Claident can be used for concatenating overlapped paired-end sequence data.
The \texttt{clconcatpair} concatenate forward and reverse sequences based on overlap positions using VSEARCH by the following command.

\begin{cmd}
{\textgreater} clconcatpair {\textbackslash}\\
{-}{-}mode=OVL {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfolder {\textbackslash}\\
outputfolder↓
\end{cmd}

This command finds \texttt{*.forward.fastq} and \texttt{*.reverse.fastq} in inputfolder, and concatenate the pairs automatically.
Gzipped \texttt{.gz} and/or bzip2ed \texttt{.bz2} files are also be found and concatenated.
Concatenated sequence files will be generated as \texttt{*.fastq.gz} in outputfolder.

If input file names are not compliant with \texttt{*.forward.fastq} and \texttt{*.reverse.fastq}, the following command can be used for concatenating a pair of files.

\begin{cmd}
{\textgreater} clconcatpair {\textbackslash}\\
{-}{-}mode=OVL {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
inputfile2 {\textbackslash}\\
outputfile↓
\end{cmd}

The forward and reverse sequence FASTQ files should be given as inputfile1 and inputfile2, respectively.
Addition of \texttt{.gz} or \texttt{.bz2} is required for output file compression.

\subsubsection{In the case of non-overlapped paired-end}

If there are no overlaps between forward and reverse sequences, quality-trimming and quality-filtering using \texttt{clfilterseq} like the following should be performed at first.

\begin{cmd}
{\textgreater} clfilterseq {\textbackslash}\\
{-}{-}minqual=30 {\textbackslash}\\
{-}{-}minquallen=3 {\textbackslash}\\
{-}{-}minlen=100 {\textbackslash}\\
{-}{-}maxplowqual=0.1 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
inputfile2 {\textbackslash}\\
outputfolder↓
\end{cmd}

The values of \texttt{{-}{-}minqual} and \texttt{{-}{-}minquallen} indicate the minimum threshold of read quality value and size of sliding window, respectively.
The above command trims 3'-tail positions until 3 bp long sequence whose read quality is 30 or higher in all 3 positions are observed.
In addition, trimmed sequences shorter than \texttt{{-}{-}minlen} will be filtered out.
The remaining sequences containing \texttt{{-}{-}maxplowqual} or more rate of lower quality positions than \texttt{{-}{-}minqual} will also be filtered out.
In this process, filtering out one of the sequence of a pair, the other sequence of the pair will also be filtered out.
The output will be generated as the same name files in outputfolder.
If you want to output to existing folder, you need to add \texttt{{-}{-}append} argument.
To apply the above command to all the pairs of \texttt{*.forward.fastq} and \texttt{*.reverse.fastq} in the current folder, execute the following commands.

\begin{cmd}
{\textgreater} for f in `ls *.forward.fastq.gz | grep -P -o '{\textasciicircum}{\lbrack}{\textasciicircum}{\textbackslash}.{\rbrack}+'`↓\\
do clfilterseq {\textbackslash}\\
{-}{-}minqual=30 {\textbackslash}\\
{-}{-}minquallen=3 {\textbackslash}\\
{-}{-}minlen=100 {\textbackslash}\\
{-}{-}maxplowqual=0.1 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
\$f.forward.fastq.gz {\textbackslash}\\
\$f.reverse.fastq.gz {\textbackslash}\\
outputfolder↓\\
done↓
\end{cmd}

After the quality-trimming and quality-filtering like above, perform sequence concatenation with the aid of \texttt{clconcatpair} like below.

\begin{cmd}
{\textgreater} clconcatpair {\textbackslash}\\
{-}{-}mode=NON {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfolder {\textbackslash}\\
outputfolder↓
\end{cmd}

In this process, the forward and reverse sequences like the following are assumed as input.

\begin{pre}
5' ― forward sequence ― 3'\\
5' ― reverse sequence ― 3'
\end{pre}

The \texttt{clconcatpair {-}{-}mode=NON} command will concatenate these sequence pairs and make sequences like the following.

\begin{pre}
5' ― reverse sequence (reverse-complement) ― ACGTACGTACGTACGT ― forward sequence ― 3'
\end{pre}

Conduct removal of noisy and/or chimeric sequences in the same way as concatenated overlapped paired-end sequence data.
In the sequence clustering by \texttt{clclassseqv} and the raw reads mapping to centroid sequences by \texttt{clrecoverseqv}, add \texttt{{-}{-}paddinglen=16} argument.
The concatenated sequences like above causes overvaluation of sequence similarity because of artificial padding sequence \texttt{ACGTACGTACGTACGT}.
The \texttt{{-}{-}paddinglen=16} argument will offset such overvaluation by exclusion of \texttt{ACGTACGTACGTACGT} from sequence similarity calculation and cluster concatenated sequences based on correct sequence similarity.

In the estimation of host organism, split concatenated sequences based on \texttt{ACGTACGTACGTACGT}, assign taxonomy to forward and reverse sequences separately.
Then, merge 2 taxonomy (see section \ref{section:mergingmultipleassignment}).
Generally speaking, forward sequences shows higher quality than reverse sequences, prefering forward sequence taxonomy is recommended if there is no \textit{a priori} infomations about identification power and variability of forward and reverse sequences.
Sequence division based on \texttt{ACGTACGTACGTACGT} can be applied to the sequences by the following command.

\begin{cmd}
{\textgreater}cldivseq {\textbackslash}\\
{-}{-}query=ACGTACGTACGTACGT {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile1 {\textbackslash}\\
outputfile2
\end{cmd}

The outputfile1 and outputfile2 contain reverse-complement of reverse sequences and forward sequences, respectively.

\subsubsection{Concatenating overlapping paired-end sequences using PEAR}

PEAR \citep{Zhang2014} can also be used for concatenation of overlapped paired-end sequences.
The following command will concatenate the pairs of sequences.

\begin{cmd}
{\textgreater} pear {\textbackslash}\\
-p 0.0001 {\textbackslash}\\
-u 0 {\textbackslash}\\
-j NumberOfCPUs {\textbackslash}\\
-f RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.forward.fastq.gz {\textbackslash}\\
-r RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.reverse.fastq.gz {\textbackslash}\\
-o RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID↓
\end{cmd}

If the processes correctly finished, the following files will be generated.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.assembled.fastq] Concatenated sequences.
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.unassembled.forward.fastq] Unconcatenated forward sequences.
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.unassembled.reverse.fastq] Unconcatenated reverse sequences.
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.discarded.fastq] Discarded sequences by statistical test.
\end{description}

Only \texttt{RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.assembled.fastq} is required in subsequent procedures.
These output files are not compressed and consume large amount of storages, compression by GZIP or BZIP2 is recommended.

To apply concatenation to all the pairs of forward and reverse sequence files by PEAR, execute the following command.

\begin{cmd}
{\textgreater} for f in `ls *.forward.fastq.gz | grep -P -o '{\textasciicircum}{\lbrack}{\textasciicircum}{\textbackslash}.{\rbrack}+'`↓\\
do pear {\textbackslash}\\
-p 0.0001 {\textbackslash}\\
-u 0 {\textbackslash}\\
-j NumberOfCPUs {\textbackslash}\\
-f \$f.forward.fastq.gz {\textbackslash}\\
-r \$f.reverse.fastq.gz {\textbackslash}\\
-o \$f↓\\
done↓
\end{cmd}

\subsubsection{Concatenating overlapping paired-end sequences using VSEARCH}

VSEARCH can also be used directly for concatenation of overlapped paired-end sequences.
The following command will concatenate the pairs of sequences.

\begin{cmd}
{\textgreater} vsearch {\textbackslash}\\
{-}{-}threads NumberOfCPUs {\textbackslash}\\
{-}{-}fastq{\textunderscore}mergepairs RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.forward.fastq.gz {\textbackslash}\\
{-}{-}reverse RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.reverse.fastq.gz {\textbackslash}\\
{-}{-}fastq{\textunderscore}allowmergestagger {\textbackslash}\\
{-}{-}fastqout RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.assembled.fastq↓
\end{cmd}

If the amplicon sequences are shorter than read length, read tail of forward sequence possibly exceeds read head of reverse sequence or read tail of reverse sequence possibly exceeds read head of forward sequence.
VSEARCH does not concatenate such sequences by default.
The \texttt{{-}{-}fastq{\textunderscore}allowmergestagger} argument enables such sequence concatenation.
The overhang positions which exceeded read head of the other sequence will be eliminated because such positions are artificial.
The \texttt{{-}{-}fastq{\textunderscore}allowmergestagger} argument is not required if there is no such sequences.
Using PEAR, the same processing as VSEARCH with \texttt{{-}{-}fastq{\textunderscore}allowmergestagger} will be performed.
The unconcatenated forward and reverse sequences can be obtained by \texttt{{-}{-}fastqout{\textunderscore}notmerged{\textunderscore}fwd outputfile} and \texttt{{-}{-}fastqout{\textunderscore}notmerged{\textunderscore}rev outputfile} arguments, respectively.
The minimum overlap length, the minimum length of concatenated sequence, the maximum length of concatenated sequence, the maximum number of allowed mismatches and the maximum allowed expected errors in concatenated sequence can be specified by \texttt{{-}{-}fastq{\textunderscore}minovlen}, \texttt{{-}{-}fastq{\textunderscore}minmergelen}, \texttt{{-}{-}fastq{\textunderscore}maxmergelen}, \texttt{{-}{-}fastq{\textunderscore}maxdiffs} and \texttt{{-}{-}fastq{\textunderscore}maxee}, respectively.
In the concatenation of overlapped paired-end sequences by \texttt{clconcatpair}, \texttt{{-}{-}fastq{\textunderscore}minovlen 20 {-}{-}fastq{\textunderscore}maxdiffs 20} is used by default, but \texttt{{-}{-}fastq{\textunderscore}minovlen 10 {-}{-}fastq{\textunderscore}maxdiffs 5} is used by default of VSEARCH.

To apply concatenation by VSEARCH to all the files in current folder, execute the following command.

\begin{cmd}
{\textgreater} for f in `ls *.forward.fastq.gz | grep -P -o '{\textasciicircum}{\lbrack}{\textasciicircum}{\textbackslash}.{\rbrack}+'`↓\\
do vsearch {\textbackslash}\\
{-}{-}threads NumberOfCPUs {\textbackslash}\\
{-}{-}fastq{\textunderscore}mergepairs \$f.forward.fastq.gz {\textbackslash}\\
{-}{-}reverse \$f.reverse.fastq.gz {\textbackslash}\\
{-}{-}fastq{\textunderscore}allowmergestagger {\textbackslash}\\
{-}{-}fastqout \$f.assembled.fastq↓\\
done↓
\end{cmd}

\subsection{Filtering potentially erroneous sequences}

There are read quality values in the FASTQ files.
Therefore, we can filter out potentially erroneous sequences using these quality values.
To do so, the following command can conduct such processing.

\begin{cmd}
{\textgreater} clfilterseq {\textbackslash}\\
{-}{-}minqual=30 {\textbackslash}\\
{-}{-}maxplowqual=0.1 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

The sequences containing \texttt{{-}{-}maxplowqual} or more rate of lower quality positions than \texttt{{-}{-}minqual} will also be filtered out by the above command.
The output is a file by default, but adding \texttt{{-}{-}output=folder} argument changes to save output as the same name file in the outputfolder.
If you want to save output file to existing folder, \texttt{{-}{-}append} argument is needed.
In the case of concatenated sequences of overlapped paired-end sequences generated by Illumina platform sequencers, positions close to the both end is usually high quality and overlapped positions is also high quality if the same positions of forward and reverse sequences are matched.
Therefore, trimming low quality positions close to the both end is needless.
Filtering out sequences containing low quality positions is recommended for concatenated overlapped paired-end sequences.
The existing sequence filtering programs such as FastQC \citep{Andrews2010} or PRINSEQ \citep{Schmieder2011} are also recommended.

To apply the same processing to the concatenated sequences by PEAR, execute the following command.

\begin{cmd}
{\textgreater} for f in *.assembled.fastq↓\\
do clfilterseq {\textbackslash}\\
{-}{-}output=folder {\textbackslash}\\
{-}{-}append {\textbackslash}\\
{-}{-}minqual=30 {\textbackslash}\\
{-}{-}maxplowqual=0.1 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
\$f {\textbackslash}\\
outputfolder↓\\
done↓
\end{cmd}

Using quality values, we can calculate expected number of read errors.
Quality-filtering based on the maximum allowed expected errors in input sequences can also be applied using VSEARCH.
The following command can apply such quality-filtering to the input sequences.

\begin{cmd}
{\textgreater} vsearch {\textbackslash}\\
{-}{-}threads NumberOfCPUs {\textbackslash}\\
{-}{-}fastq{\textunderscore}filter inputfile {\textbackslash}\\
{-}{-}fastq{\textunderscore}maxee 1.0 {\textbackslash}\\
{-}{-}fastqout outputfile↓
\end{cmd}

For the single-end sequence data and unconcatenated sequence data, same quality-trimming and quality-filtering as Roche GS series sequencers and Ion PGM can be recommended (see section \ref{subsection:qualityfilteringfor454}).
Note that thresholds for quality values and read lengths should be changed.

\section{If you sequenced same PCR amplicons multiply or replicated PCR amplicons of same templates}

Because chimeric sequences are constructed in each PCR tube, whether the sequences are came from same tube or not should be given to analysis programs.
Therefore, several procedure is required to give such information to the programs in the cases of replicated sequencing of same PCR amplicons and sequencing of replicated PCR amplicons of same templates.

\subsection{In the case of sequencing of replicated PCR amplicons of same templates using same tags in the same run}

In this case, chimera removal based on replicates of PCR cannot be applied.
This case can treat as same as unreplicated PCR.

\subsection{In the case of sequencing of replicated PCR amplicons of same templates using different tags in the same run}

In Claident, RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID is used as sample IDs.
Therefore, there are multiple samples from the same templates in this case.
It may be good idea that common sequences among replicated samples are treated as noiseless and nonchimeric, and uncommon sequences are treated as noisy and/or chimeric sequences if noise occurrence and chimera formation can be assumed as random.
However, noise occurrence and chimera formation likely to be nonrandom.
Noisy and/or chimeric sequences might be occurred across all replicates.
The effectiveness of this method does not confirmed enough, and combination of noisy and/or chimeric sequence removal based on PCR replicates and algorithms can be recommended (see also \citet{Lange2015}).
Such combination is supported in Claident, it is explained later.
One of the final output of Claident is a sample x OTU table containing the number of sequences in every cell.
The table modification command \texttt{clfiltersum} can be used for integration of multiple samples to one, and cell numbers will be summed up.

\subsection{In the case of sequencing of same PCR amplicons using same tags in different runs}

In Claident, RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID is used as sample IDs.
Therefore, there are multiple samples from the same templates in this case.
Such samples can be treated as separate or integrated to one sample.
It is recommended that separate samples have been used subsequent analysis without any change, all filtered and denoised sequences of multiple runs are given to clustering commands, and such samples are finally integrated in a sample x OTU table.
The table generation command \texttt{clsumclass} with \texttt{{-}{-}runname} argument or table modification command \texttt{clfiltersum} with \texttt{{-}{-}runname} argument can be used to replace RunIDs and to integrate multiple samples to one, and cell numbers will be summed up.

If you want to integrate multiple samples from the same templates at this time, execute the following commands.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
*snip* {\textbackslash}\\
{-}{-}runname=FOO {\textbackslash}\\
inputfile1 {\textbackslash}\\
outputfolder↓\\
{\textgreater} clsplitseq {\textbackslash}\\
*snip* {\textbackslash}\\
{-}{-}runname=FOO {\textbackslash}\\
{-}{-}append {\textbackslash}\\
inputfile2 {\textbackslash}\\
outputfolder↓
\end{cmd}

The inputfile1 and inputfile2 are the FASTQ files of first and second run, respectively.
The \texttt{{-}{-}runname} argument replaces RunID of the sequence names to \texttt{FOO}.
Thus, all the sequences of both input files will be saved to \texttt{FOO{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.fastq.gz} in the output folder.

In Claident, the sequences whose names contain the same RunID, TagID and PrimerID are treated as the sequences from the same samples.
If tag and primer sequence files are the same, TagID and PrimerID of output sequences are the same.
Replacing RunID to \texttt{FOO}, all of RunID, TagID and PrimerID become the same in the sequences from the same samples.

\subsection{In the case of sequencing of replicated PCR amplicons of same templates using same tags in different runs}

You should have multiple FASTQ files.
Run demultiplexing by \texttt{clsplitseq} and quality-trimming and quality-filtering by \texttt{clfilterseq} separately, and save processed sequences to different folders.
After separate processing of noisy and/or chimeric sequence removal, give all processed sequence files to clustering programs at once.
One of the final output of Claident is a sample x OTU table containing the number of sequences in every cell.
The table modification command \texttt{clfiltersum} can be used for integration of multiple samples to one, and cell numbers will be summed up.

\subsection{In the case of sequencing of replicated PCR amplicons of same templates using different tags in different runs}

In Claident, RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID is used as sample IDs.
Therefore, there are multiple replicated samples from the same templates in this case.
Such samples can be treated as separate or integrated to one sample.
It is recommended that separate samples have been used subsequent analysis without any change and such samples are finally integrated in a sample x OTU table.
It may be good idea that common sequences among replicated samples are treated as noiseless and nonchimeric, and uncommon sequences are treated as noisy and/or chimeric sequences if noise occurrence and chimera formation can be assumed as random.
Noisy and/or chimeric sequences might be occurred across all replicates.
The effectiveness of this method does not confirmed enough, and combination of noisy and/or chimeric sequence removal based on PCR replicates and algorithms can be recommended (see also \citet{Lange2015}).
Such combination is supported in Claident, it is explained later.
One of the final output of Claident is a sample x OTU table containing the number of sequences in every cell.
The table modification command \texttt{clfiltersum} can be used for integration of multiple samples to one, and cell numbers will be summed up.

In the demultiplexing using \texttt{clsplitseq}, process multiple FASTQ files like the following.

\begin{cmd}
{\textgreater} clsplitseq {\textbackslash}\\
*snip* {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile1 {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
inputfile1 {\textbackslash}\\
outputfolder↓\\
{\textgreater} clsplitseq {\textbackslash}\\
*snip* {\textbackslash}\\
{-}{-}tagfile=TagSequenceFile2 {\textbackslash}\\
{-}{-}primerfile=PrimerSequenceFile {\textbackslash}\\
{-}{-}append {\textbackslash}\\
inputfile2 {\textbackslash}\\
outputfolder↓
\end{cmd}

The sequence file of primer and the output folder should be the same.
The different RunID should be given for \texttt{{-}{-}runname} argument.
The different tag sequence files should be prepared and given.
Note that the same TagID should be specified for the replicated samples from the same templates even if tag/index sequence is different.
In addition, different TagID should be specified for the samples from different templates even if tag/index sequence is the same.
In the case of the following tag sequence files, the sequences of sample1 were added \texttt{ACGTACGT} as a tag in the first run, and \texttt{ATGCATGC} in the second run.

\begin{content}
| {\textgreater}sample1\\
| ACGTACGT\\
| {\textgreater}sample2\\
| ATGCATGC
\end{content}

\begin{content}
| {\textgreater}sample1\\
| ATGCATGC\\
| {\textgreater}sample2\\
| ACGTACGT
\end{content}

The sequences of sample2 were added \texttt{ATGCATGC} as a tag in the first run, and \texttt{ACGTACGT} in the second run.
The subsequent analysis is explained later.

\chapter{Noisy and/or chimeric sequence removal}

Claident can detect noisy sequences containing read errors and/or copy errors based on sequence abandance.
This method is similar to the method implemented in CD-HIT-OTU \citep{Li2012}.
In the old pipeline using Assams for dereplication and clustering, chimera removal based on UCHIME \citep{Edgar2011} algorithm can be applied.
Chimera removal based on UCHIME \citep{Edgar2011} algorithm is applied after OTU picking in the new pipeline using VSEARCH for dereplication and clustering.

Run the following command to perform noisy sequence detection and removal.
Note that multiple input files can be given, and all sequence files of the same run should be given at once because sequencing quality varied among different runs.
If there are too many sequences and the processing requires long time, give one sequence file and run the following command several times.

\begin{cmd}
{\textgreater} clcleanseqv {\textbackslash}\\
{-}{-}derepmode=PREFIX {\textbackslash}\\
{-}{-}primarymaxnmismatch=0 {\textbackslash}\\
{-}{-}secondarymaxnmismatch=1 {\textbackslash}\\
{-}{-}pnoisycluster=0.5 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
*snip* {\textbackslash}\\
inputfileN {\textbackslash}\\
outputfolder↓
\end{cmd}

Whether full-length perfect matching (\texttt{FULLLENGTH}) or prefix search (\texttt{PREFIX}) applied in dereplication is given for the \texttt{{-}{-}derepmode} argument.
\texttt{FULLLENGTH} is recommended for concatenated sequences of overlapped paired-end sequencing.
\texttt{PREFIX} is recommended for single-end or concatenated sequences of non-overlapped paired-end sequencing.
The \texttt{{-}{-}primarymaxnmismatch} argument indicates the number of mismatches in primary clustering and 0 is recommended for the most cases.
The \texttt{{-}{-}secondarymaxnmismatch} argument describes the number of mismatches in secondary clustering and 1 is recommended for the most cases.
For the noisy data, use \texttt{{-}{-}primarymaxnmismatch=1 {-}{-}secondarymaxnmismatch=3} or \texttt{{-}{-}primarymaxnmismatch=2 {-}{-}secondarymaxnmismatch=5}.
Twice as \texttt{{-}{-}primarymaxnmismatch} plus one should be specified for \texttt{{-}{-}secondarymaxnmismatch}.
The \texttt{{-}{-}pnoisycluster} argument determines sensitivity of noise detection.
Decimal value larger than 0 and smaller than 1 must be specified for this argument.
The larger value acquire higher sensitivity.
If you use 97\% identity cutoff in clustering, 0.5 is recommended and this is the default value.
If you use 99\% or larger identity cutoff in clustering, 0.9 or larger value might be more suitable.
The larger value causes exclusion of more low abundance sequences, more sequences should be obtained per sample.

The following files should be saved in the output folder.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[parameter.txt] The minimum size of primary clusters remained
\item[primarycluster.denoised.fasta.gz] Representative sequences of primary clusters determined as non-noisy
\item[primarycluster.fasta.gz] Representative sequences of primary clusters
\item[secondarycluster.fasta.gz] Representative sequences of secondary clusters
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.noisyreads.txt.gz] List of sequences determined as noisy
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.singletons.txt.gz] List of singletons after primary clustering
\end{description}

Many other files might be generated and do not delete such files because those might be required in subsequent analysis.

\section{Noisy and/or chimeric sequence removal based on PCR replicates}

To detect and remove noisy and/or chimeric sequences using PCR replicates, which samples are from the same templates need to be provided as a text file like below.

\begin{content}
| sample1~ sample2~ sample3\\
| sample4~ sample5\\
| sample6~ sample7
\end{content}

Tab-delimited sample list in a line indicates samples from the same templates.
Samples from different templates must be placed in different lines.
3 or more replicates are allowed.
The number of replicates can vary among templates.

The above file are prepared, the following command removes uncommon primary cluster as noisy and/or chimeric.

\begin{cmd}
{\textgreater} clcleanseqv {\textbackslash}\\
{-}{-}replicatelist=ListOfPCRreplicates {\textbackslash}\\
{-}{-}derepmode=PREFIX {\textbackslash}\\
{-}{-}primarymaxnmismatch=0 {\textbackslash}\\
{-}{-}secondarymaxnmismatch=1 {\textbackslash}\\
{-}{-}pnoisycluster=0.5 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
*snip* {\textbackslash}\\
inputfileN {\textbackslash}\\
outputfolder↓
\end{cmd}

If 3 or more replicates are available, only common sequences among all replicates are determined as non-noisy and nonchimeric.
If a primary cluster occured among multiple templates and the cluster determined as noisy or chimeric in a template, the cluster determined as noisy or chimeric in the other templates and will be excluded from all the samples by default.
However, the following arguments can change this decision method.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[\texttt{{-}{-}minnreplicate}] Specified as integer larger than 1.
If the number of replicates occured is equal to or larger than this value, such primary cluster will be determined as non-noisy and nonchimeric.
The default value is \texttt{2}.
\item[\texttt{{-}{-}minpreplicate}] Specified as decimal larger than 0.
If the proportion of replicates occured (number of replicates occured / total number of replicates of the same template) is equal to or larger than this value, such primary cluster will be determined as non-noisy and nonchimeric.
The default value is \texttt{1}.
\item[\texttt{{-}{-}minnpositive}] Specified as integer larger than 0.
If the number of sequences determined as noisy or chimeric of a primary cluster is equal to or larger than this value, the cluster determined as noisy or chimeric in all the samples.
The default value is \texttt{1}.
\item[\texttt{{-}{-}minppositive}] Specified as decimal equal to or larger than 0.
If the proportion of sequences determined as noisy or chimeric of a primary cluster (number of sequences determined as noisy or chimeric of a primary cluster / total number of sequences of a primary cluster) is equal to or larger than this value, the cluster determined as noisy or chimeric in all the samples.
The default value is \texttt{0}.
\item[\texttt{{-}{-}runname}] Specify RunID.
RunIDs in all sample names will be replaced to this RunID.
If multiple sample names become to the same, such samples will be integrated.
\end{description}

The \texttt{{-}{-}minnreplicate} and \texttt{{-}{-}minpreplicate} are arguments about intrasample judgement.
The \texttt{{-}{-}minnpositive} and \texttt{{-}{-}minppositive} are arguments about intersample judgement.
If both of \texttt{{-}{-}minnreplicate} and \texttt{{-}{-}minpreplicate} are fullfilled, the primary cluster will be determined as non-noisy and nonchimeric.
If both of \texttt{{-}{-}minnpositive} and \texttt{{-}{-}minppositive} are fullfilled, the primary cluster will be determined as noisy or chimeric.
The decision is common among all samples and different decision among samples are not allowed.
If there are samples that is not written in replicate list file, such sample is not used in this decision.

If \texttt{clcleanseqv} is executed like above, the following files will be saved additionally.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[primarycluster.chimeraremoved.fasta.gz] Representative sequences determined as nonchimeric.
\item[primarycluster.cleaned.fasta.gz] Representative sequences determined as non-noisy and nonchimeric
\item[RunID{\textunderscore}{\textunderscore}TagID{\textunderscore}{\textunderscore}PrimerID.chimericreads.txt.gz] List of sequences determined as chimeric
\end{description}

\chapter{OTU picking based on nucleotide sequence clustering}

\section{Inter-sample clustering}

To pick OTUs by clustering, run the following command.

\begin{cmd}
{\textgreater} clclassseqv {\textbackslash}\\
{-}{-}minident=0.97 {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
*snip* {\textbackslash}\\
inputfileN {\textbackslash}\\
outputfolder↓
\end{cmd}

Give \texttt{primarycluster.cleaned.fasta.gz} (if you applied noisy and/or chimeric sequence removal using PCR replicates) or \texttt{primarycluster.denoised.fasta.gz} (if you did not apply noisy and/or chimeric sequence removal using PCR replicates) which is generated by \texttt{clcleanseqv} as input files.
If you are using non-overlapped paired-end sequence data concatenated by \texttt{clconcatpair}, give \texttt{{-}{-}paddinglen=16} argument additionally.

In the output folder, the following files should be saved.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[clustered.otu.gz] Compressed file which records affiliation of raw sequences to OTUs
\item[clustered.fasta] Representative sequences of OTUs
\end{description}

\section{Mapping raw sequencing reads to representative sequences of the clusters}

If there are the raw sequences determined as noisy or chimeric which are as similar or more similar to representative sequences than specified identity threshold, such raw sequences can be recovered in this step.
This process can decrease excluded sequences.

To perform this process, run the following command.

\begin{cmd}
{\textgreater} clrecoverseqv {\textbackslash}\\
{-}{-}minident=0.97 {\textbackslash}\\
{-}{-}centroid=RepresentativeSequenceFile {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile1 {\textbackslash}\\
*snip* {\textbackslash}\\
inputfileN {\textbackslash}\\
outputfolder↓
\end{cmd}

Specify \texttt{clustered.fasta} generated by \texttt{clclassseqv} and \texttt{primarycluster.fasta.gz} generated by \texttt{clcleanseqv} as representative sequence file and input file, respectively.
If you are using non-overlapped paired-end sequence data concatenated by \texttt{clconcatpair}, give \texttt{{-}{-}paddinglen=16} argument additionally.
In the output folder, the output files same as \texttt{clclassseqv} will be generated.

\chapter{Summarizing and post-processing of OTU picking results}

\section{Making summary table}\label{section:makesummarytable}

The following command generates a sample x OTU table containing the number of sequences in every cell from OTU picking results.

\begin{cmd}
{\textgreater} clsumclass {\textbackslash}\\
{-}{-}output=Matrix {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

Give \texttt{clustered.otu.gz} generated by \texttt{clclassseqv} or \texttt{clrecoverseqv} as input file.
The output file is tab-delimited text file like Table \ref{table:exampletableofsummary} and can be edit by spreadsheet softwares such as Microsoft Excel.
Note that spreadsheet software can not read too large table.
This file can be used for community ecological analysis in R.

\begin{table}[h]
\begin{center}
\footnotesize\setlength{\baselineskip}{0.9em}%
\begin{tabular}{l|rrrrrr} 
samplename & OTU1 & OTU2 & OTU3 & OTU4 & OTU5 & OTU6 \\\hline\hline
sampleA & 2371 & 0 & 0 & 12 & 3 & 0 \\
sampleB & 0 & 1518 & 0 & 25 & 0	& 1 \\
sampleC & 1398 & 0 & 0 & 8 & 77 & 6 \\
sampleD & 0 & 1436 & 0 & 10 & 0	& 0 \\
sampleE & 0 & 0 & 1360 & 0 & 15 & 3 \\
sampleF & 0 & 0 & 977 & 55 & 6 & 8 \\
\end{tabular}
\end{center}
\caption{An example of summary table --- Numbers in cells indicate the observed number of raw sequences.}
\label{table:exampletableofsummary}
\end{table}

\section{Excluding specified OTUs and/or samples from summary table}\label{section:clfiltersum}

The \texttt{clsumclass} command output all OTUs and samples to summary table.
The following command can filter samples and/or OTUs which matches several conditions.

\begin{cmd}
{\textgreater} clfiltersum {\textbackslash}\\
arguments {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

Both input file and output file are tab-delimited text files of summary tables.
Acceptable arguments are listed below.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[\texttt{{-}{-}minnseqotu}] Specified as integer as large as or larger than 0.
OTUs whose number of raw sequencing reads of every sample is lower than this value will be excluded.
\item[\texttt{{-}{-}minpseqotu}] Specified as decimal ranging from 0 to 1.
OTUs whose proportion of raw sequencing reads (number of raw reads / total number of raw reads of sample) of every sample is lower than this value will be excluded.
\item[\texttt{{-}{-}minntotalseqotu}] Specified as integer as large as or larger than 0.
OTUs whose total number of raw sequencing reads is lower than this value will be excluded.
\item[\texttt{{-}{-}minnseqsample}] Specified as integer as large as or larger than 0.
Samples whose number of raw sequencing reads of every OTU is lower than this value will be excluded.
\item[\texttt{{-}{-}minpseqsample}] Specified as decimal ranging from 0 to 1.
Samples whose proportion of raw sequencing reads (number of raw reads / total number of raw reads of OTU) of every OTU is lower than this value will be excluded.
\item[\texttt{{-}{-}minntotalseqsample}] Specified as integer as large as or larger than 0.
Samples whose total number of raw sequencing reads is lower than this value will be excluded.
\item[\texttt{{-}{-}otu}] Specify OTUs as comma-delimited names you want to keep.
\item[\texttt{{-}{-}negativeotu}] Specify OTUs as comma-delimited names you want to eliminate.
\item[\texttt{{-}{-}otulist}] Specify a file name.
In the file, write an OTU name per line you want to keep.
\item[\texttt{{-}{-}negativeotulist}] Specify a file name.
In the file, write an OTU name per line you want to eliminate.
\item[\texttt{{-}{-}otuseq}] Specify a FASTA sequence file name.
In the file, write OTU names you want to keep. Sequences will be ignored.
\item[\texttt{{-}{-}negativeotuseq}] Specify a FASTA sequence file name.
In the file, write OTU names you want to eliminate. Sequences will be ignored.
\item[\texttt{{-}{-}sample}] Specify samples as comma-delimited names you want to keep.
\item[\texttt{{-}{-}negativesample}] Specify samples as comma-delimited names you want to eliminate.
\item[\texttt{{-}{-}samplelist}] Specify a file name.
In the file, write a sample name per line you want to keep.
\item[\texttt{{-}{-}negativesamplelist}] Specify a file name.
In the file, write a sample name per line you want to eliminate.
\item[\texttt{{-}{-}replicatelist}] Specify a file name.
In the file, write PCR replicates as tab-delimited sample names in a line.
PCR replicates will be integrated in output file.
The number of raw reads will be summed up.
\item[\texttt{{-}{-}runname}] Specify RunID.
RunIDs in all sample names will be replaced this RunID.
If samples whose names are completely matched are occur, the samples will be integrated in output file.
The number of raw reads will be summed up.
\end{description}

Applying post OTU picking chimeric sequence removal or nontarget sequence removal explained later, OTU filtering using remaining sequences and \texttt{{-}{-}otuseq} argument can be applied to summary table.
If you want to apply additional filtering to summary table, this OTU filtering should be applied at first.

\section{Chimera removal based on UCHIME algorithm}

Post OTU picking chimera removal based on UCHIME algorithm with and without (\textit{de novo}) reference sequences can be applied additionally.
If you want to apply both with and without reference chimera removal, without reference (\textit{de novo}) chimera removal should be applied at first, and then reference-based chimera removal should be applied.
To perform these chimera removal, use \texttt{clrunuchime} command.
In this command, UCHIME algorithm implemented in VSEARCH is used.

\subsection{\textit{De novo} chimera removal}

Execute \texttt{clrunuchime} like the following.

\begin{cmd}
{\textgreater} clrunuchime {\textbackslash}\\
{-}{-}otufile=*.otu.gz {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

The output files in output folder is explained later.

\subsection{Reference-based chimera removal}

Execute \texttt{clrunuchime} like the following.

\begin{cmd}
{\textgreater} clrunuchime {\textbackslash}\\
{-}{-}referencedb=ReferenceDatabase {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

If you already applied \textit{de novo} chimera removal, there is \texttt{nonchimeras.fasta} in the output folder of \textit{de novo} chimera removal.
This file is recommended for input file of reference-based chimera removal.
The output files in output folder is explained later.

The following ready-made reference databases are provided and installed.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[cdu12s] Claident Database for UCHIME for animal 12S ver.20180412
\item[cdu16s] Claident Database for UCHIME for animal 16S ver.20180412
\item[cducox1] Claident Database for UCHIME for animal COX1(COI) ver.20180412
\item[cducytb] Claident Database for UCHIME for animal Cyt-b ver.20180412
\item[cdudloop] Claident Database for UCHIME for animal D-loop ver.20180412
\item[cdumatk] Claident Database for UCHIME for plant matK ver.20180412
\item[cdurbcl] Claident Database for UCHIME for plant rbcL ver.20180412
\item[cdutrnhpsba] Claident Database for UCHIME for plant trnH-psbA ver.20180412
\item[rdpgoldv9] RDP Gold v9 for prokaryotic 16S
\item[silva132LSUref] SILVA Release 132 for LSU rRNA
\item[silva132SSUrefnr99] SILVA Release 132 Nr99 for SSU rRNA
\item[unite20170628] UNITE ver.20170628 for fungal ITS
\item[unite20170628untrim] UNITE ver.20170628 without trimming for fungal ITS
\item[unite20170628its1] UNITE ver.20170628 for fungal ITS1
\item[unite20170628its2] UNITE ver.20170628 for fungal ITS2
\end{description}

\subsection{About contents of output folder}

In the output folder of \texttt{clrunuchime}, the following files will be saved.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[chimeras.fasta] Sequences determined as chimeric
\item[nonchimeras.fasta] Sequences determined as nonchimeric
\item[uchimealns.txt] Alignment used in chimera detection
\item[uchimeout.txt] Detected parent sequences, chimera scores and the other information
\end{description}

To know meaning of each element in \texttt{uchimeout.txt}, see the following URL.\\
\href{http://drive5.com/usearch/manual/uchimeout.html}{http://drive5.com/usearch/manual/uchimeout.html}

\section{Excluding low-abundance OTUs from OTU sequences}

In the output folder of OTU picking, \texttt{clustered.fasta} is saved as representative sequence file.
This file can be used for taxonomic assignment.
However, the number of OTUs is sometimes too large to assign taxonomy if rare OTUs are kept.
In such cases, extracting more abundant OTUs than specified value is useful.
The following command exclude OTUs observed less than 5.

\begin{cmd}
{\textgreater} clfilterseq {\textbackslash}\\
{-}{-}otufile=*.otu.gz {\textbackslash}\\
{-}{-}minnseq=5 {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

\section{Sequence splitting based on conservative motif recognition}

If your data sequences contain multiple loci (e.g. ITS1--5.8S rRNA--ITS2), splitting loci might cause better taxonomic assignment.
If conservative motif exists at the border of loci or close to the border, such motif can be used for splitting.
Universal primer annealing positions are recommended for such conservative motif.
The following command can divide sequences to anterior and posterior of matched positions.

\begin{cmd}
{\textgreater} cldivseq {\textbackslash}\\
{-}{-}query=Sequence {\textbackslash}\\
{-}{-}border=start {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile1 {\textbackslash}\\
outputfile2↓
\end{cmd}

By default, given query sequence will be searched from the sequences contained in inputfile based on Needleman-Wunsch algorithm allowing 15\% mismatches.
If matched positions are found, sequences will be divided on head of matched positions to anterior and posterior sequences to outputfile1 and outputfile2, respectively.
Unmatched sequences will be output to outputfile1.
Given query sequence must be same strand as target sequences.
To use different strand query, add \texttt{{-}{-}reversecomplement} argument.

If \texttt{{-}{-}border=end} is specified, sequences will be divided on tail of matched positions.
If \texttt{{-}{-}border=both} is given (this is default), anterior sequences of head of matched positions and posterior sequences of tail of matched positions will be saved, and matched positions will be excluded from both output.
If query is unmatched, undivided sequences will be saved to outputfile1 and outputfile2 will lack the sequences.
Specifying the \texttt{{-}{-}makedummy} argument, dummy sequence \texttt{A} will be saved to outputfile2.
This argument is useful for merging multiple taxonomic assignment results of multiple loci.
Repeat \texttt{cldivseq} execution to divide sequences to 3 or more subsequences.

\section{ITS, SSU rRNA or LSU rRNA sequence extraction using ITSx or Metaxa}

ITSx can detect ITS1 and ITS2, and extract those positions only \citep{Bengtsson2013}.
In many taxa, ITS is highly variable but SSU, 5.8S and LSU rRNA is much more conservative.
Such conservative positions cause misidentifications or unidentified results because conservative position of distant taxa will match to query sequences.
Extracting ITS positions by ITSx might solve this problem.

Metaxa can detect SSU (12S/16S/18S) rRNA and LSU (26S/28S) rRNA, and extract those positions only \citep{Bengtsson2011}.
In the case of SSU rRNA, Metaxa can also distinguish among eukaryotic nuclear, mitochondrial, chloroplast and prokaryotic.
SSU rRNA is widely used for DNA barcoding and metabarcoding of eukaryotes, but is contained not only by nuclear of eukaryotes but also by mitochondria, chloroplast and contaminated prokaryotes.
Therefore, nontarget SSU rRNA frequently contaminate the data.
Such nontarget SSU should be deleted for community ecological analysis.

After the filtering sequences by ITSx or Metaxa, \texttt{clfiltersum} with \texttt{{-}{-}otuseq} argument can be used to justify summary table to the sequence file (see section \ref{section:clfiltersum}).

\section{Searching and excluding nontarget sequences}

ITSx and Metaxa can apply to ITS and SSU/LSU rRNA only and cannot apply to the other loci.
For the other loci, gene prediction and annotation programs and multiple alignment programs can be used for finding nontarget sequences.
In ClustalW2, ClustalX2 \citep{Larkin2007} and MAFFT \citep{Katoh2013}, sorting function based on phylogenetic relatedness is available.
Aligning sequences applying this sorting function, you can easily find some types of nontarget sequences by eyes with the aid of multiple alignment viewer.
After the filtering sequences by this method, \texttt{clfiltersum} with \texttt{{-}{-}otuseq} argument can be used to justify summary table to the sequence file (see section \ref{section:clfiltersum}).

\chapter{Data deposition to DRA}

Representative sequences can be deposited to DDBJ, EMBL or GenBank if required, but raw sequencing reads should be deposited to Sequence Read Archive such as DDBJ Sequence Read Archive (DRA).
If you sequenced multiple samples with tag sequences in a NGS run like above, demultiplexed (tag and primer positions are trimmed) sequence files should be deposited.
FASTQ files made by \texttt{clsplitseq} command can be used for deposition.

XML files recoding metadata about sample information need to be created.
DRA provides XML creation support tool, but such tools is not easy-to-use in a case of many samples.
To reduce time and effort to make XML, \texttt{clmaketsv} and \texttt{clmakexml} can be used.

To deposit raw sequences to DRA, user account of D-way of DDBJ and public key registration are required.
Read DRA Handbook\\
\href{http://trace.ddbj.nig.ac.jp/dra/submission_e.shtml}{http://trace.ddbj.nig.ac.jp/dra/submission{\textunderscore}e.shtml}\\
to know detailed procedures.
In the deposition process, concept of Submission, Study, Experiment, Sample and Run, and association of those are important and need to be understood.

In DRA, Study need to be registered to BioProject database which is a research project database, and to be referred to BioProject ID (accession number).
Read BioProject Handbook\\
\href{http://trace.ddbj.nig.ac.jp/bioproject/submission_e.html}{http://trace.ddbj.nig.ac.jp/bioproject/submission{\textunderscore}e.html}\\
and register Study to BioProject before data deposition to DRA.

Sample also need to be registered to BioSample database which is a biological specimen database, and to be referred to BioSample ID (accession number).
You need to read BioSample Handbook\\
\href{http://trace.ddbj.nig.ac.jp/biosample/submission_e.html}{http://trace.ddbj.nig.ac.jp/biosample/submission{\textunderscore}e.html}\\
and to register Sample to BioSample before data deposition to DRA.
In the cases of metabarcoding using amplicon sequences amplified by universal primer set, MIMarks-Survey should be specified as type of MIxS.
In BioSample database, sampling locality, elevation, depth, temperature, humidity, pH etc. can be added as metadata.
Sample information should be added as many as possible for future generations and yourself.
Meaning of each item is explained in checklist\\
\href{http://wiki.gensc.org/index.php?title=MIMARKS}{http://wiki.gensc.org/index.php?title=MIMARKS}\\
provided by Genomic Standards Consortium.
If you cannot understand meaning of each item, ask to DDBJ.
In the cases of underground samples and underseafloor samples, hight or depth of sampling point from mean sea level, hight or depth of sampling point from ground/seafloor surface and hight or depth of ground/seafloor surface from mean sea level need to be distinguished.
Because it is very confusing, please be careful.

\section{Preparing tab-delimited text file for XML generation}

In the XML files, contained sequence infomation need to be provided for each FASTQ files.
Because this is very costful, \texttt{clmaketsv} generates simple tab-delimited text file, and \texttt{clmakexml} creates XML files from edited tab-delimited text file.
The \texttt{clmaketsv} command can run like below.

\begin{cmd}
{\textgreater} clmaketsv {\textbackslash}\\
inputfile1 {\textbackslash}\\
*snip* {\textbackslash}\\
inputfileN {\textbackslash}\\
outputfile↓
\end{cmd}

FASTQ files to deposit to DRA should be specified as input files.
Wild cards can be used in input file name.
Once tab-delimited text generated, use spreadsheet softwares such as Microsoft Excel to edit this file, and fill in the blank cells.
Note that Microsoft Excel automatically convert several types of characters and this function cannot be disabled.
If \texttt{{\lbrack}Foo,Bar{\rbrack}} is found in a cell, select \texttt{Foo} or \texttt{Bar} and delete bracket, comma and alternatives.
If you find \texttt{{\textless}Fill in this cell{\textgreater}} in a cell, fill this cell according to written instruction in \texttt{{\textless}{\textgreater}} and delete \texttt{{\textless}{\textgreater}}.
You might do something with the other cells.
If you do not have BioProject ID and/or BioSample ID because of assignment delay, you can use Submission ID with prefix PSUB for BioProject and SSUB for BioSample.

\section{XML generation from tab-delimited text}

After editing tab-delimited text, run \texttt{clmakexml} like below.

\begin{cmd}
{\textgreater} clmakexml {\textbackslash}\\
Tab-DelimitedTextFile {\textbackslash}\\
Submission-ID↓
\end{cmd}

Then, XML files required to deposit FASTQ files will be created.
Multiple tab-delimited text files can be given to \texttt{clmakexml}.
In such cases, the contents of first 3 lines in tab-delimited text of secondary or n-ary files will be ignored.

Submission-ID is assigned by DRA when [Create new submission(s)] button is pushed.
Submission-ID is compliant with \texttt{UserID-0001} and should be given for last argument of \texttt{clmakexml}.
The execution of \texttt{clmakexml} is completed, 3 \texttt{Submission-ID.*.xml} files will be generated.
These files can be upload to DRA using XML Upload.
All processes were completed, DRA accession number is assigned and is sent from DRA to your Email address.
Note that BioProject ID (accession number) was written in manuscript in most cases.

\chapter{Estimation of host organisms of nucleotide sequences (a.k.a. DNA barcoding)}

DNA barcoding is a taxonomic identification method of biological specimens using nucleotide sequences and becoming widely applied to broad area.
However, our reference sequence database is not enough for species level identification and algorithm that is suitable for incomplete reference database is lacking.
To solve this problem, I proposed a new criterion that sequence distance between query and nearest-neighbor must be smaller than maximum distance within resulting taxon, developed QCauto algorithm which fullfills this criterion \citep{Tanabe2013} and implemented in Claident.
Note that we can expect that host organism of query is same as nearest-neighbor in the case that potentially observable species are completely described and barcode DNA sequences are completely sequenced and registered to reference database.
For such case, 1-nearest-neighbor (closest match) method is also implemented in Claident.

\section{Retrieval of neighborhood sequences based on BLAST search}

The GenBank IDs of neighborhood sequences required to fullfill ``sequence distance between query and nearest-neighbor must be smaller than maximum distance within resulting taxon'' can be retrieved by the following command.

\begin{cmd}
{\textgreater} clidentseq {\textbackslash}\\
{-}{-}blastdb=overall{\textunderscore}genus {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

Give FASTA formatted nucleotide sequence file as an input file.
Specify reference sequence database for BLAST search to \texttt{{-}{-}blastdb} argument.
The following databases should be installed with Claident.

\begin{description}\small\setlength{\baselineskip}{1.1em}
\item[animals{\textunderscore}COX1{\textunderscore}genus] Animal (Metazoa) \textit{COX1} sequences which have genus or lower level taxonomic information
\item[animals{\textunderscore}COX1{\textunderscore}species] Animal (Metazoa) \textit{COX1} sequences which have species or lower level taxonomic information
\item[animals{\textunderscore}mt{\textunderscore}genus] Animal (Metazoa) mitochondrial sequences which have genus or lower level taxonomic information
\item[animals{\textunderscore}mt{\textunderscore}species] Animal (Metazoa) mitochondrial sequences which have species or lower level taxonomic information
\item[eukaryota{\textunderscore}LSU{\textunderscore}genus] Eukaryotic LSU (28S) rRNA sequences which have genus or lower level taxonomic information
\item[eukaryota{\textunderscore}LSU{\textunderscore}species] Eukaryotic LSU (28S) rRNA sequences which have species or lower level taxonomic information
\item[eukaryota{\textunderscore}SSU{\textunderscore}genus] Eukaryotic SSU (18S) rRNA sequences which have genus or lower level taxonomic information
\item[eukaryota{\textunderscore}SSU{\textunderscore}species] Eukaryotic SSU (18S) rRNA sequences which have species or lower level taxonomic information
\item[fungi{\textunderscore}ITS{\textunderscore}genus] Fungal ITS sequences which have genus or lower level taxonomic information
\item[fungi{\textunderscore}ITS{\textunderscore}species] Fungal ITS sequences which have species or lower level taxonomic information
\item[overall{\textunderscore}class] NCBI nt sequences which have class or lower level taxonomic information
\item[overall{\textunderscore}order] NCBI nt sequences which have order or lower level taxonomic information
\item[overall{\textunderscore}family] NCBI nt sequences which have family or lower level taxonomic information
\item[overall{\textunderscore}genus] NCBI nt sequences which have genus or lower level taxonomic information
\item[overall{\textunderscore}species] NCBI nt sequences which have species or lower level taxonomic information
\item[plants{\textunderscore}matK{\textunderscore}genus] Plant \textit{matK} sequences which have genus or lower level taxonomic information
\item[plants{\textunderscore}matK{\textunderscore}species] Plant \textit{matK} sequences which have species or lower level taxonomic information
\item[plants{\textunderscore}rbcL{\textunderscore}genus] Plant \textit{rbcL} sequences which have genus or lower level taxonomic information
\item[plants{\textunderscore}rbcL{\textunderscore}species] Plant \textit{rbcL} sequences which have species or lower level taxonomic information
\item[plants{\textunderscore}trnH-psbA{\textunderscore}genus] Plant \textit{trnH}--\textit{psbA} sequences which have genus or lower level taxonomic information
\item[plants{\textunderscore}trnH-psbA{\textunderscore}species] Plant \textit{trnH}--\textit{psbA} sequences which have species or lower level taxonomic information
\item[prokaryota{\textunderscore}16S{\textunderscore}genus] Prokaryotic (Bacterial and Archaeal) 16S rRNA sequences which have genus or lower level taxonomic information
\item[prokaryota{\textunderscore}16S{\textunderscore}species] Prokaryotic (Bacterial and Archaeal) 16S rRNA sequences which have species or lower level taxonomic information
\item[prokaryota{\textunderscore}all{\textunderscore}genus] Prokaryotic (Bacterial and Archaeal) sequences which have genus or lower level taxonomic information
\item[prokaryota{\textunderscore}all{\textunderscore}species] Prokaryotic (Bacterial and Archaeal) sequences which have species or lower level taxonomic information
\item[semiall{\textunderscore}class] Reduced database excluding all sequences of vertebrates, \textit{Caenorhabditis} and \textit{Drosophila} from \texttt{overall{\textunderscore}class}
\item[semiall{\textunderscore}order] Reduced database excluding all sequences of vertebrates, \textit{Caenorhabditis} and \textit{Drosophila} from \texttt{overall{\textunderscore}order}
\item[semiall{\textunderscore}family] Reduced database excluding all sequences of vertebrates, \textit{Caenorhabditis} and \textit{Drosophila} from \texttt{overall{\textunderscore}family}
\item[semiall{\textunderscore}genus] Reduced database excluding all sequences of vertebrates, \textit{Caenorhabditis} and \textit{Drosophila} from \texttt{overall{\textunderscore}genus}
\item[semiall{\textunderscore}species] Reduced database excluding all sequences of vertebrates, \textit{Caenorhabditis} and \textit{Drosophila} from \texttt{overall{\textunderscore}species}
\end{description}

The \texttt{overall{\textunderscore}*} databases are very large and require large memory but are versatile and robust to contamination of sequences of unexpected taxa or loci.
Thus, the \texttt{overall{\textunderscore}*} are strongly recommended in most cases.
In the \texttt{overall{\textunderscore}genus} database, the sequences which have genus or lower level taxonomic information are kept and the other sequences are excluded and nearest-neighbor often cannot be found in minor taxa.
The \texttt{overall{\textunderscore}class} database is recommended for such cases.
The other databases are much smaller than \texttt{overall{\textunderscore}*} and require much smaller memory.

\subsection{Accelerating BLAST search using cache databases}

The \texttt{clidentseq} command runs BLAST search multiple times and requires long time.
To reduce runtime of \texttt{clidentseq}, cache sequence database containing top-10,000 high score sequences can be constructed for each query sequence.
Processing speed of \texttt{clidentseq} can be extremely accelerated by using cache database and strongly recommended.
To construct cache database, run \texttt{clmakecachedb} like the following.

\begin{cmd}
{\textgreater} clmakecachedb {\textbackslash}\\
{-}{-}blastdb=overall{\textunderscore}genus {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfolder↓
\end{cmd}

In the execution of \texttt{clidentseq}, give the output folder of \texttt{clmakecachedb} to \texttt{{-}{-}blastdb} argument, then cache database will be used.
Note that the input files of \texttt{clmakecachedb} and \texttt{clidentseq} must be tha same file.
Required amount of memory is also extremely reduced.

\subsection{In the case of perfect reference database}\label{subsection:completereferencedatabase}

If potentially observable species are completely described and barcode DNA sequences are completely sequenced and registered to reference database, retrieving top-1 and tie sequences whose percent-identity to query is 99\% or more is recommended and it can be performed by the following command.

\begin{cmd}
{\textgreater} clidentseq {\textbackslash}\\
blastn -task megablast -word{\textunderscore}size 16 end {\textbackslash}\\
{-}{-}method=1,99\% {\textbackslash}\\
{-}{-}blastdb=overall{\textunderscore}genus {\textbackslash}\\
{-}{-}numthreads=NumberOfCPUs {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

In search of top-1 and tie sequences whose percent-identity to query is 99\% or more, we can expect that overleaping hardly occur even if \texttt{-task megablast -word{\textunderscore}size 16} is given as BLAST search argument, thus this argument is specified.

\section{Taxonomic assignment based on neighborhood sequences}

The following command assigns taxonomy to query ascending taxonomic level until taxonomic information of neighborhood sequences are completely matched.
This is called as lowest common ancestor (LCA) algorithm \citep{Huson2007}.

\begin{cmd}
{\textgreater} classigntax {\textbackslash}\\
{-}{-}taxdb=overall{\textunderscore}genus {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

Give the output file of \texttt{clidentseq} as input file.
The \texttt{{-}{-}taxdb} argument is to give taxonomy database of reference sequences.
The same name taxonomy databases as reference sequence databases should be installed and specified.

The \texttt{classigntax} requires 2 or more neighborhood sequences by default.
If \texttt{{-}{-}method=1,99\%} was given to \texttt{clidentseq} like section \ref{subsection:completereferencedatabase}, taxonomic assignment of all sequences must be failed.
In such case, reduce required minimum number of neighborhood sequences like below.

\begin{cmd}
{\textgreater} classigntax {\textbackslash}\\
{-}{-}taxdb=overall{\textunderscore}genus {\textbackslash}\\
{-}{-}minnsupporter=1 {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

The output file of \texttt{classigntax} is tab-delimited text file like Table \ref{table:exampleresultsofidentification}.

\begin{table}[h]
\begin{center}
\footnotesize\setlength{\baselineskip}{0.9em}%
\begin{tabular}{l|lll} 
query & phylum & genus & species \\\hline\hline
seqA & Ascomycota & \textit{Chloridium} & \textit{Chloridium virescens} \\
seqB & Ascomycota & \textit{Chloridium} & \textit{Chloridium virescens} \\
seqC & Ascomycota & \textit{Chloridium} & \textit{Chloridium virescens} \\
seqD & Basidiomycota & \textit{Amanita} & \textit{Amanita fuliginea} \\
seqE & Basidiomycota & \textit{Coltriciella} & \textit{Coltriciella dependens} \\
seqF & Basidiomycota & \textit{Filobasidium} & \textit{Filobasidium uniguttulatum} \\
seqG & Basidiomycota & \textit{Laccaria} & \textit{Laccaria bicolor} \\
seqH & Basidiomycota & \textit{Lactarius} & \textit{Lactarius quietus} \\
seqI & Basidiomycota & \textit{Russula} & \textit{Russula densifolia} \\
seqJ & Basidiomycota & \textit{Russula} & \textit{Russula densifolia} \\
seqK & Basidiomycota & \textit{Russula} & \textit{Russula densifolia} \\
seqL & Basidiomycota & \textit{Russula} & \textit{Russula vesca} \\
seqM & Basidiomycota & \textit{Agaricus} &  \\
seqN & Basidiomycota & \textit{Amanita} &  \\
seqO & Basidiomycota & \textit{Amanita} &  \\
seqP & Ascomycota & \textit{Bisporella} &  \\
seqQ & Ascomycota & \textit{Capronia} &  \\
seqR & Ascomycota & \textit{Capronia} &  \\
seqS & Ascomycota & \textit{Cenococcum} &  \\
\end{tabular}
\end{center}
\caption{Examples of taxonomic assignment --- Blank cel means unidentified. Several taxonomic levels are omitted for reduce width.}
\label{table:exampleresultsofidentification}
\end{table}

By default, the \texttt{classigntax} command assigns taxonomy to query ascending taxonomic level until taxonomic information of neighborhood sequences are completely matched.
If reference database is contaminated by misidentified sequence and such sequence is contain in neighborhoods, lower level taxonomy likely to be unidentified.
This ``strict consensus'' method is robust but too conservative in some cases.
The following command allows 5\% of neighborhood sequences have different taxonomic information to resulting taxonomy.

\begin{cmd}
{\textgreater} classigntax {\textbackslash}\\
{-}{-}taxdb=overall{\textunderscore}genus {\textbackslash}\\
{-}{-}maxpopposer=0.05 {\textbackslash}\\
{-}{-}minsoratio=19 {\textbackslash}\\
inputfile {\textbackslash}\\
outputfile↓
\end{cmd}

The \texttt{classigntax} treat neighborhood sequences which have same taxonomic information to resulting taxonomy as ``supporter'', neighborhood sequences which have different taxonomic information to resulting taxonomy as ``opposer''.
The \texttt{{-}{-}maxpopposer} argument is to specify maximum allowing proportion of opposer sequences.
The \texttt{{-}{-}minsoratio} argument is to set minimum required ratio of ``supporter / opposer''.
Because there are possibly unidentified sequences which do not have taxonomic information and such sequences are neither supporter nor opposer, the above 2 arguments are required.
In the case of above command, 5\% of opposers are tolerated and 19 times as many supporters as opposers are required.

\section{Merging multiple taxonomic assignments based on consensus}\label{section:mergingmultipleassignment}

In plant chloroplast loci, single locus query sequences cannot be identified at the lower taxonomic level in many cases because of lack of variations.
Merging multiple results based on \texttt{overall{\textunderscore}genus} (more reliable database) and \texttt{overall{\textunderscore}class} (less reliable database) are often useful.
Merging multiple results based on ``strict consensus'' LCA (strict LCA) and ``95\%-majority rule consensus'' LCA (relaxed LCA) is also useful in some cases.
Such merging multiple assignment results can be performed with the aid of \texttt{clmergeassign}.

Multiple assignment results of \textit{rbcL} and \textit{matK} can be merged prefering lower level identification by the following command.

\begin{cmd}
{\textgreater} clmergeassign {\textbackslash}\\
{-}{-}priority=equal {\textbackslash}\\
{-}{-}preferlower {\textbackslash}\\
ResultOfrbcL {\textbackslash}\\
ResultOfmatK {\textbackslash}\\
outputfile↓
\end{cmd}

The following command merges multiple results accepting that query identified as same taxon in both results and that query identified in one result and unidentified in the other result.
Note that mismatch at the higher level taxonomy is not allowed.

\begin{cmd}
{\textgreater} clmergeassign {\textbackslash}\\
{-}{-}priority=equal {\textbackslash}\\
ResultOfrbcL {\textbackslash}\\
ResultOfmatK {\textbackslash}\\
outputfile↓
\end{cmd}

Three assignment results of \textit{rbcL}, \textit{matK} and \textit{trnH}--\textit{psbA} can be merged prefering lower level identification by the following command.

\begin{cmd}
{\textgreater} clmergeassign {\textbackslash}\\
{-}{-}priority=equal {\textbackslash}\\
{-}{-}preferlower {\textbackslash}\\
ResultOfrbcL {\textbackslash}\\
ResultOfmatK {\textbackslash}\\
ResultOftrnH-psbA {\textbackslash}\\
outputfile↓
\end{cmd}

The following command merges two results prefering result of \texttt{overall{\textunderscore}genus}.

\begin{cmd}
{\textgreater} clmergeassign {\textbackslash}\\
{-}{-}priority=descend {\textbackslash}\\
ResultOfoverall{\textunderscore}genus {\textbackslash}\\
ResultOfoverall{\textunderscore}class {\textbackslash}\\
outputfile↓
\end{cmd}

In this case, result of \texttt{overall{\textunderscore}class} is accepted if matched to result of \texttt{overall{\textunderscore}genus} and identified at the lower level than result of \texttt{overall{\textunderscore}genus}.

The following command merges two results prefering result of strict LCA.

\begin{cmd}
{\textgreater} clmergeassign {\textbackslash}\\
{-}{-}priority=descend {\textbackslash}\\
ResultOfStrictLCA {\textbackslash}\\
ResultOfRelaxedLCA {\textbackslash}\\
outputfile↓
\end{cmd}

In this case, result of relaxed LCA is accepted if matched to result of strict LCA and identified at the lower level than result of strict LCA.

\bibliography{metabarcodingtextbook.en}
\addcontentsline{toc}{chapter}{References}

\appendix

\chapter{Instllation of optional programs}

\section{Installation of bcl2fastq}

Bcl2fastq is a program to convert BCL formatted basecall data to FASTQ provided by Illumina.
Bcl2fastq is available at\\
\href{http://support.illumina.com/downloads/bcl2fastq_conversion_software.html}{http://support.illumina.com/downloads/bcl2fastq{\textunderscore}conversion{\textunderscore}software.html}

Bcl2fastq v1.8.4 for Illumina sequencing systems using versions of RTA earlier than RTA version 1.18.54 can be installed like below.

\begin{cmd}
\# Install alien\\
\$ sudo apt-get install alien\\
\# Download bcl2fastq v1.8.4 from Illumina\\
\$ wget {\textbackslash}\\
ftp://webdata:webdata@ussd-ftp.illumina.com/Downloads/Software/bcl2fastq/bcl2fastq-1.8.4-Linux-x86{\textunderscore}64.rpm\\
\# Convert .rpm to .deb and install .deb\\
\$ sudo alien -i {\textbackslash}\\
bcl2fastq-1.8.4-Linux-x86{\textunderscore}64.rpm\\
\# Install required packages\\
\$ sudo apt-get install libxml-simple-perl xsltproc
\end{cmd}

In newer version of Ubuntu, incompatible Perl to bcl2fastq is installed.
To solve this problem, run the following commands.

\begin{cmd}
\$ sudo perl -i -npe {\textbackslash}\\
's/qw{\textbackslash}(ELAND{\textunderscore}FASTQ{\textunderscore}FILES{\textunderscore}PER{\textunderscore}PROCESS{\textbackslash})/{\textbackslash}("ELAND{\textunderscore}FASTQ{\textunderscore}FILES{\textunderscore}PER{\textunderscore}PROCESS"{\textbackslash})/' {\textbackslash}\\
/usr/local/lib/bcl2fastq-1.8.4/perl/Casava/Alignment/Config.pm\\
\$ sudo perl -i -npe {\textbackslash}\\
's/qw{\textbackslash}(ELAND{\textunderscore}GENOME{\textbackslash})/{\textbackslash}("ELAND{\textunderscore}GENOME"{\textbackslash})/' {\textbackslash}\\
/usr/local/lib/bcl2fastq-1.8.4/perl/Casava/Alignment/Config.pm
\end{cmd}

You can also use text editor program to edit line 747 and line 751 of\\
\texttt{/usr/local/lib/bcl2fastq-1.8.4/perl/Casava/Alignment/Config.pm}\\
and replace \texttt{qw(FOO)} to \texttt{("FOO")}.

To install bcl2fastq2 v2.20.0.422 for all Illumina sequencing systems running RTA version 1.18.54 and above, run the following commands.

\begin{cmd}
\# Install rpm2cpio and cpio\\
{\textgreater} sudo apt-get install rpm2cpio cpio↓\\
\# Download bcl2fastq2 v2.20.0.422 from Illumina\\
{\textgreater} wget -O bcl2fastq2-v2-20-0-linux-x86-64.zip https://bit.ly/2NEy9qG↓\\
\# Decompress zip file\\
{\textgreater} unzip -qq bcl2fastq2-v2-20-0-linux-x86-64.zip↓\\
\# Extract executable command from .rpm\\
{\textgreater} rpm2cpio {\textbackslash}\\
bcl2fastq2-v2.20.0.422-Linux-x86{\textunderscore}64.rpm | cpio -id↓\\
\# Install executable command\\
{\textgreater} sudo mv usr/local/bin/bcl2fastq /usr/local/bin/↓\\
\# Install the other resources\\
{\textgreater} sudo cp -R usr/local/share/css /usr/local/share/↓\\
{\textgreater} sudo cp -R usr/local/share/xsl /usr/local/share/↓
\end{cmd}

\chapter{Terminal command examples}\label{chapter:terminalcommands}

\section{Counting sequences in a file}

\begin{cmd}
\# Count seqs in FASTQ\\
{\textgreater} grep -P -c '{\textasciicircum}{\textbackslash}+{\textbackslash}r?{\textbackslash}n?\$' inputfile↓\\
\# Count seqs in gzipped FASTQ\\
{\textgreater} gzip -dc inputfile {\textbar} grep -P -c '{\textasciicircum}{\textbackslash}+{\textbackslash}r?{\textbackslash}n?\$'↓\\
\# Count seqs in bzip2ed FASTQ\\
{\textgreater} bzip2 -dc inputfile {\textbar} grep -P -c '{\textasciicircum}{\textbackslash}+{\textbackslash}r?{\textbackslash}n?\$'↓\\
\# Count seqs in xzed FASTQ\\
{\textgreater} xz -dc inputfile {\textbar} grep -P -c '{\textasciicircum}{\textbackslash}+{\textbackslash}r?{\textbackslash}n?\$'↓\\
\# Count seqs in FASTA\\
{\textgreater} grep -P -c '{\textasciicircum}{\textgreater}' inputfile↓\\
\# Count seqs in gzipped FASTA\\
{\textgreater} gzip -dc inputfile {\textbar} grep -P -c '{\textasciicircum}{\textgreater}'↓\\
\# Count seqs in bzip2ed FASTA\\
{\textgreater} bzip2 -dc inputfile {\textbar} grep -P -c '{\textasciicircum}{\textgreater}'↓\\
\# Count seqs in xzed FASTA\\
{\textgreater} xz -dc inputfile {\textbar} grep -P -c '{\textasciicircum}{\textgreater}'↓
\end{cmd}

\section{Viewing sequence files}

\begin{cmd}
\# Output contents of uncompressed file to screen\\
{\textgreater} cat inputfile↓\\
\# View contents of uncompressed file\\
{\textgreater} less inputfile↓\\
\# Output contents of gzipped file to screen\\
{\textgreater} gzip -dc inputfile↓\\
\# View contents of gzipped file\\
{\textgreater} gzip -dc inputfile {\textbar} less↓\\
\# Extract first seq from FASTQ and output to screen\\
{\textgreater} head -n 4 inputfile↓\\
\# Extract first seq from gzipped FASTQ and output to screen\\
{\textgreater} gzip -dc inputfile {\textbar} head -n 4↓\\
\# Extract matched seqs to regular expression in seq name from FASTQ and output to screen\\
{\textgreater} grep -P -A 3 '{\textasciicircum}{\textbackslash}@RegularExpressionOfSequenceName' inputfile↓\\
\# Extract matched seqs to regular expression in seq name from gzipped FASTQ and output to screen\\
{\textgreater} gzip -dc inputfile {\textbar} grep -P -A 3 '{\textasciicircum}{\textbackslash}@RegularExpressionOfSequenceName'↓
\end{cmd}

\section{Compression and decompression}

\begin{cmd}
\# Decompress all .fastq.gz files in current folder\\
{\textgreater} for f in *.fastq.gz↓\\
do gzip -d \$f↓\\
done↓\\
\# Decompress all .fastq.gz files in current folder and subfolders\\
{\textgreater} for f in `find . -name *.fastq.gz`↓\\
do gzip -d \$f↓\\
done↓\\
\# Decompress all .fastq.gz files in current folder using 4 CPUs\\
{\textgreater} ls *.fastq.gz | xargs -L 1 -P 4 gzip -d↓\\
\# Decompress all .fastq.gz files in current folder and subfolders using 4 CPUs\\
{\textgreater} find . -name *.fastq.gz | xargs -L 1 -P 4 gzip -d↓\\
\# Compress all .fastq files in current folder\\
{\textgreater} for f in *.fastq↓\\
do gzip \$f↓\\
done↓\\
\# Compress all .fastq files in current folder and subfolders\\
{\textgreater} for f in `find . -name *.fastq`↓\\
do gzip \$f↓\\
done↓\\
\# Compress all .fastq files in current folder using 4 CPUs\\
{\textgreater} ls *.fastq | xargs -L 1 -P 4 gzip↓\\
\# Compress all .fastq files in current folder and subfolders using 4 CPUs\\
{\textgreater} find . -name *.fastq | xargs -L 1 -P 4 gzip↓
\end{cmd}

\section{Extraction and output of sequences}

\begin{cmd}
\# Extract first 10,000 seqs from FASTQ and save those seqs to a file\\
{\textgreater} head -n 40000 inputfile {\textgreater} outputfile↓\\
\# Extract last 10,000 seqs from FASTQ and save those seqs to a file\\
{\textgreater} tail -n 40000 inputfile {\textgreater} outputfile↓\\
\# Extract first 10,000 seqs from gzipped FASTQ and save those seqs to gzipped FASTQ\\
{\textgreater} gzip -dc inputfile {\textbar} head -n 40000 {\textbar} gzip -c {\textgreater} outputfile↓\\
\# Extract last 10,000 seqs from gzipped FASTQ and save those seqs to gzipped FASTQ\\
{\textgreater} gzip -dc inputfile {\textbar} tail -n 40000 {\textbar} gzip -c {\textgreater} outputfile↓\\
\# Extract matched seqs to regular expression in seq name from gzipped FASTQ and save to gzipped FASTQ\\
{\textgreater} gzip -dc inputfile {\textbar} grep -P -A 3 '{\textasciicircum}{\textbackslash}@RegularExpressionOfSequenceName' {\textbar} gzip -c {\textgreater} outputfile↓
\end{cmd}

\end{document}
